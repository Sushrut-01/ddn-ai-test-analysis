# CRAG Verification Layer - Complete Guide

**Version**: 1.4.0
**Date**: 2025-11-02
**Status**: Production Ready

---

## Table of Contents

1. [Introduction](#introduction)
2. [Architecture Overview](#architecture-overview)
3. [Multi-Dimensional Confidence Scoring](#multi-dimensional-confidence-scoring)
4. [Confidence Thresholds and Routing](#confidence-thresholds-and-routing)
5. [Self-Correction Workflow](#self-correction-workflow)
6. [Human-in-the-Loop (HITL) Process](#human-in-the-loop-hitl-process)
7. [Web Search Fallback](#web-search-fallback)
8. [Metrics and Monitoring](#metrics-and-monitoring)
9. [Integration Guide](#integration-guide)
10. [Usage Examples](#usage-examples)
11. [Configuration](#configuration)
12. [Troubleshooting](#troubleshooting)
13. [Performance Characteristics](#performance-characteristics)

---

## Introduction

CRAG (Corrective Retrieval Augmented Generation) is a verification layer that ensures AI-generated error analysis results are accurate, well-grounded, and trustworthy. It acts as a quality gate between the ReAct agent's analysis and the final response to users.

### What CRAG Does

- **Verifies** AI-generated answers using multi-dimensional confidence scoring
- **Routes** answers based on confidence levels to appropriate workflows
- **Self-corrects** low-confidence answers through query expansion and re-retrieval
- **Escalates** uncertain answers to human reviewers (HITL)
- **Augments** very low confidence answers with web search results
- **Monitors** system performance through comprehensive metrics

### Why CRAG?

Traditional RAG systems have a fundamental problem: they don't know when they're wrong. CRAG solves this by:

1. **Quantifying uncertainty** through 5-dimensional confidence scoring
2. **Taking action** based on confidence levels (correct, escalate, or search)
3. **Learning** from patterns through metrics and HITL feedback

---

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        ReAct Agent Analysis                      ‚îÇ
‚îÇ                  (root_cause + fix_recommendation)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     CRAG Verifier                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ          Multi-Dimensional Confidence Scorer               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Relevance (0.25)    ‚Ä¢ Consistency (0.20)              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Grounding (0.25)    ‚Ä¢ Completeness (0.15)             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Classification (0.15)                                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                            ‚îÇ                                      ‚îÇ
‚îÇ                            ‚ñº                                      ‚îÇ
‚îÇ              Calculate Overall Confidence                         ‚îÇ
‚îÇ                            ‚îÇ                                      ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ         ‚îÇ                                     ‚îÇ                  ‚îÇ
‚îÇ    Confidence >= 0.85?              Confidence >= 0.65?          ‚îÇ
‚îÇ         ‚îÇ YES                              ‚îÇ YES                 ‚îÇ
‚îÇ         ‚ñº                                  ‚ñº                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ PASS        ‚îÇ                    ‚îÇ HITL Queue   ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ (60-70%)    ‚îÇ                    ‚îÇ (20-30%)     ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ         ‚îÇ NO                              ‚îÇ NO                   ‚îÇ
‚îÇ         ‚îÇ                                 ‚îÇ                      ‚îÇ
‚îÇ    Confidence >= 0.40?                    ‚îÇ                      ‚îÇ
‚îÇ         ‚îÇ YES                             ‚îÇ                      ‚îÇ
‚îÇ         ‚ñº                                 ‚ñº                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ Self-Correction  ‚îÇ            ‚îÇ Web Search     ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ (10-15%)         ‚îÇ            ‚îÇ Fallback       ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Query Expansion  ‚îÇ            ‚îÇ (5-10%)        ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Verified Answer + Metadata                    ‚îÇ
‚îÇ  ‚Ä¢ status: PASS | HITL | CORRECTED | WEB_SEARCH                ‚îÇ
‚îÇ  ‚Ä¢ confidence: 0.0 - 1.0                                        ‚îÇ
‚îÇ  ‚Ä¢ confidence_level: HIGH | MEDIUM | LOW | VERY_LOW             ‚îÇ
‚îÇ  ‚Ä¢ action_taken: pass_through | queued_for_hitl | etc.         ‚îÇ
‚îÇ  ‚Ä¢ verification_metadata: {...}                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Components

1. **ConfidenceScorer**: Calculates multi-dimensional confidence scores
2. **CRAGVerifier**: Main orchestrator that routes based on confidence
3. **SelfCorrector**: Query expansion and re-retrieval for low confidence
4. **HITLManager**: PostgreSQL-backed queue for human review
5. **WebSearchFallback**: External search integration for very low confidence
6. **CRAGMetrics**: Comprehensive performance monitoring

---

## Multi-Dimensional Confidence Scoring

CRAG doesn't rely on a single confidence score. Instead, it evaluates answers across 5 dimensions:

### 1. Relevance Score (Weight: 0.25)

**What it measures**: How relevant are the retrieved documents to the question?

**Calculation**: Average similarity scores of retrieved documents from Pinecone.

**Example**:
```python
docs = [
    {'similarity_score': 0.92},
    {'similarity_score': 0.88},
    {'similarity_score': 0.85}
]
relevance_score = (0.92 + 0.88 + 0.85) / 3 = 0.883
```

**Interpretation**:
- **> 0.85**: Highly relevant documents retrieved
- **0.65 - 0.85**: Moderately relevant
- **< 0.65**: Low relevance, may need broader search

### 2. Consistency Score (Weight: 0.20)

**What it measures**: Do the retrieved documents agree with each other?

**Calculation**: Pairwise term overlap between documents using Jaccard similarity.

**Example**:
```python
doc1 = "Update TOKEN_EXPIRATION to 3600 seconds"
doc2 = "Set token expiration to one hour (3600s)"
# High overlap in key terms ‚Üí high consistency
```

**Interpretation**:
- **> 0.70**: Documents tell the same story
- **0.40 - 0.70**: Some agreement, some variation
- **< 0.40**: Conflicting information

### 3. Grounding Score (Weight: 0.25)

**What it measures**: Is the generated answer supported by retrieved documents?

**Calculation**: For each sentence in the answer, check if key terms appear in retrieved docs.

**Example**:
```python
answer = "Update TOKEN_EXPIRATION in auth/middleware.py to 3600"
docs_text = "TOKEN_EXPIRATION setting in auth/middleware.py controls token lifetime"
# Key terms found: TOKEN_EXPIRATION, auth/middleware.py ‚Üí well-grounded
```

**Interpretation**:
- **> 0.80**: Answer is well-supported by documents
- **0.50 - 0.80**: Partial grounding, some hallucination
- **< 0.50**: Significant hallucination risk

**This is the most important anti-hallucination metric.**

### 4. Completeness Score (Weight: 0.15)

**What it measures**: Does the answer contain all required components for its error category?

**Required Components by Category**:

| Category | Required Components |
|----------|-------------------|
| CODE_ERROR | root_cause, fix_steps, code_location, verification |
| INFRA_ERROR | root_cause, fix_steps, verification |
| CONFIG_ERROR | root_cause, fix_steps, config_location, verification |
| DEPENDENCY_ERROR | root_cause, fix_steps, verification |
| TEST_ERROR | root_cause, fix_steps, verification |
| UNKNOWN | root_cause, fix_steps |

**Example (CODE_ERROR)**:
```python
answer = {
    'root_cause': 'Token expiration in auth/middleware.py line 45',  # ‚úì
    'fix_recommendation': 'Update TOKEN_EXPIRATION and run tests',    # ‚úì (fix_steps + verification)
    # File path found ‚Üí code_location ‚úì
}
# Score: 4/4 = 1.0 (complete)
```

### 5. Classification Confidence (Weight: 0.15)

**What it measures**: How confident was the ReAct agent in its error classification?

**Source**: Directly from ReAct agent's `classification_confidence` field.

**Interpretation**:
- **> 0.90**: Very confident classification
- **0.70 - 0.90**: Confident classification
- **< 0.70**: Uncertain classification

---

### Overall Confidence Calculation

The overall confidence is a **weighted average** of the 5 component scores:

```python
overall_confidence = (
    0.25 * relevance_score +
    0.20 * consistency_score +
    0.25 * grounding_score +
    0.15 * completeness_score +
    0.15 * classification_confidence
)
```

**Why these weights?**

- **Relevance (0.25)**: Most important for retrieval quality
- **Grounding (0.25)**: Most important for avoiding hallucinations
- **Consistency (0.20)**: Important for conflicting information
- **Completeness (0.15)**: Important but can vary by category
- **Classification (0.15)**: Contributes but shouldn't dominate

---

## Confidence Thresholds and Routing

CRAG uses 4 confidence thresholds to route answers to appropriate workflows:

### HIGH Confidence (‚â• 0.85)

**Action**: Pass through immediately ‚úì

**Expected**: 60-70% of cases

**Characteristics**:
- High document relevance (‚â• 0.85)
- Strong grounding (minimal hallucination)
- Complete answer with all required components
- ReAct classification confident

**Example**:
```json
{
  "status": "PASS",
  "confidence": 0.92,
  "confidence_level": "HIGH",
  "action_taken": "pass_through",
  "answer": { /* verified ReAct result */ }
}
```

**Response Time**: < 50ms (just scoring)

---

### MEDIUM Confidence (0.65 - 0.85)

**Action**: Queue for Human-in-the-Loop (HITL) review ‚ö†Ô∏è

**Expected**: 20-30% of cases

**Characteristics**:
- Moderate document relevance
- Some uncertainty in answer
- May be missing minor details
- Worth human review before deployment

**Workflow**:
1. Calculate priority (high/medium/low) based on:
   - Confidence level (closer to 0.65 = higher priority)
   - Error category (INFRA/CONFIG = high priority)
   - Environment (production = higher priority)

2. Queue in PostgreSQL `hitl_queue` table with:
   - Unique failure_id
   - Confidence scores and concerns
   - SLA deadline (2 hours for high priority)
   - Priority level

3. Send notification to review team (Slack/Teams webhook)

4. Return provisional answer with `review_url`

**Example**:
```json
{
  "status": "HITL",
  "confidence": 0.72,
  "confidence_level": "MEDIUM",
  "action_taken": "queued_for_hitl",
  "answer": { /* provisional answer */ },
  "review_url": "/review/BUILD-12345",
  "verification_metadata": {
    "queue_id": 123,
    "priority": "medium",
    "sla_deadline": "2025-11-02T16:00:00",
    "concerns": ["Moderate confidence", "Missing verification steps"]
  }
}
```

**Response Time**: < 100ms (scoring + queue insert)

---

### LOW Confidence (0.40 - 0.65)

**Action**: Attempt self-correction through query expansion ‚Üª

**Expected**: 10-15% of cases

**Characteristics**:
- Low document relevance (docs don't match well)
- Incomplete answer
- Potential for improvement with better retrieval

**Self-Correction Process** (see detailed section below):

1. Identify low-scoring components
2. Expand query with category-specific terms
3. Re-retrieve from Pinecone (both indexes)
4. Re-analyze with expanded context
5. Re-score confidence

**Success Criteria**: New confidence ‚â• 0.65 (target: 60% success rate)

**If Successful**:
```json
{
  "status": "CORRECTED",
  "confidence": 0.68,
  "confidence_level": "CORRECTED",
  "action_taken": "self_correction",
  "answer": { /* improved answer */ },
  "verification_metadata": {
    "original_confidence": 0.55,
    "new_confidence": 0.68,
    "improvement": 0.13,
    "method": "query_expansion"
  }
}
```

**If Failed**: Escalate to HITL with note about correction attempt

**Response Time**: 500ms - 2s (re-retrieval + re-analysis)

---

### VERY LOW Confidence (< 0.40)

**Action**: Web search fallback üåê

**Expected**: 5-10% of cases

**Characteristics**:
- Very low document relevance (RAG failed completely)
- Poor classification confidence
- Likely an error type not in our knowledge base

**Web Search Process** (see detailed section below):

1. Extract technical terms from error message
2. Generate search query for error + fix
3. Query web search APIs (Google ‚Üí Bing ‚Üí DuckDuckGo)
4. Extract and rank search result snippets
5. Enhance answer with web information
6. Estimate new confidence based on search quality

**Success Criteria**: Improved confidence with credible sources (target: 50% success rate)

**If Successful**:
```json
{
  "status": "WEB_SEARCH",
  "confidence": 0.62,
  "confidence_level": "WEB_ENHANCED",
  "action_taken": "web_search_fallback",
  "answer": { /* enhanced with web info */ },
  "verification_metadata": {
    "original_confidence": 0.35,
    "new_confidence": 0.62,
    "web_sources": [
      {"url": "https://stackoverflow.com/q/12345", "title": "Fix auth error"}
    ],
    "search_engine": "google",
    "search_query": "authentication error TOKEN_EXPIRATION fix"
  }
}
```

**If Failed**: Escalate to HITL with **high priority** + web search details

**Response Time**: 1-3s (external API call)

---

## Self-Correction Workflow

Self-correction attempts to improve low-confidence answers (0.40 - 0.65) through query expansion and re-retrieval.

### When Self-Correction Triggers

- Confidence between 0.40 and 0.65
- One or more component scores are low
- SelfCorrector available (graceful degradation if not)

### Self-Correction Algorithm

```
1. IDENTIFY LOW COMPONENTS
   ‚îú‚îÄ Which dimensions scored < 0.60?
   ‚îú‚îÄ relevance low ‚Üí need better docs
   ‚îú‚îÄ grounding low ‚Üí need more specific info
   ‚îî‚îÄ completeness low ‚Üí need more detail

2. EXPAND QUERY (Category-Specific)
   ‚îú‚îÄ CODE_ERROR ‚Üí add: "code fix solution implementation"
   ‚îú‚îÄ INFRA_ERROR ‚Üí add: "infrastructure troubleshoot setup"
   ‚îú‚îÄ CONFIG_ERROR ‚Üí add: "configuration settings setup"
   ‚îú‚îÄ DEPENDENCY_ERROR ‚Üí add: "install package dependency"
   ‚îî‚îÄ TEST_ERROR ‚Üí add: "test fix assertion mock"

3. RE-RETRIEVE (Dual-Index RAG)
   ‚îú‚îÄ Query knowledge_docs index (error documentation)
   ‚îú‚îÄ Query error_library index (past cases)
   ‚îî‚îÄ Merge results (top 5 from each)

4. RE-SCORE CONFIDENCE
   ‚îú‚îÄ Calculate new confidence with expanded docs
   ‚îú‚îÄ Check if improvement ‚â• MIN_IMPROVEMENT (0.05)
   ‚îî‚îÄ Check if new confidence ‚â• TARGET_CONFIDENCE (0.65)

5. DECIDE
   ‚îú‚îÄ If improved AND ‚â• 0.65 ‚Üí SUCCESS (return corrected answer)
   ‚îú‚îÄ If max retries (2) reached ‚Üí FAIL (escalate to HITL)
   ‚îî‚îÄ Otherwise ‚Üí try again with different expansion
```

### Example: Successful Self-Correction

**Original State**:
```json
{
  "confidence": 0.55,
  "components": {
    "relevance": 0.58,      // LOW - docs don't match well
    "consistency": 0.52,    // LOW
    "grounding": 0.55,      // LOW
    "completeness": 0.48,   // LOW - missing details
    "classification": 0.62
  }
}
```

**After Query Expansion** (`"authentication error" ‚Üí "authentication error code fix solution TOKEN_EXPIRATION"`):

```json
{
  "confidence": 0.68,
  "components": {
    "relevance": 0.72,      // IMPROVED - better docs found
    "consistency": 0.65,    // IMPROVED
    "grounding": 0.68,      // IMPROVED
    "completeness": 0.70,   // IMPROVED - more complete answer
    "classification": 0.62  // UNCHANGED
  },
  "improvement": 0.13,
  "attempts": 1
}
```

**Result**: SUCCESS - return corrected answer

### Self-Correction Metrics

Track these metrics to monitor effectiveness:
- **Success rate**: Target 60%
- **Average improvement**: Typical 0.10 - 0.15
- **Average attempts**: Should be < 1.5
- **Time cost**: Adds 500ms - 2s per verification

---

## Human-in-the-Loop (HITL) Process

HITL provides human oversight for medium-confidence answers (0.65 - 0.85) that need expert review.

### HITL Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      PostgreSQL Database                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ              hitl_queue table                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ id (serial primary key)                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ failure_id (unique)                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ react_result (JSONB)                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ confidence (float)                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ confidence_scores (JSONB)                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ concerns (JSONB array)                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ priority (high/medium/low)                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ status (pending/approved/rejected)                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ sla_deadline (timestamp)                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ created_at, updated_at                                 ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Review Dashboard                             ‚îÇ
‚îÇ  ‚Ä¢ View pending reviews (sorted by priority + SLA)              ‚îÇ
‚îÇ  ‚Ä¢ See confidence breakdown and concerns                        ‚îÇ
‚îÇ  ‚Ä¢ Approve or reject with feedback                              ‚îÇ
‚îÇ  ‚Ä¢ Track review history                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Queue Process

#### 1. Queueing for Review

When MEDIUM confidence detected:

```python
queue_item = hitl_manager.queue(
    react_result=react_result,
    confidence=0.72,
    confidence_scores=scores,
    failure_data=failure_data,
    priority='medium'
)
```

**Priority Calculation**:
- **High**: Confidence < 0.70 OR INFRA_ERROR OR CONFIG_ERROR
- **Medium**: Confidence 0.70-0.85 AND CODE_ERROR
- **Low**: Other cases

**SLA Deadlines**:
- **High priority**: 2 hours
- **Medium priority**: 8 hours
- **Low priority**: 24 hours

#### 2. Notification

Webhook sent to review team:

```json
{
  "type": "hitl_review_required",
  "priority": "medium",
  "failure_id": "BUILD-12345",
  "confidence": 0.72,
  "concerns": ["Moderate confidence", "Missing verification steps"],
  "sla_deadline": "2025-11-02T16:00:00",
  "review_url": "https://dashboard.example.com/review/BUILD-12345"
}
```

#### 3. Human Review

Reviewer sees:
- Original error message and context
- ReAct agent's analysis
- Confidence breakdown (5 components)
- Similar past cases
- Suggested fixes

Reviewer actions:
- **Approve**: Analysis is correct ‚Üí mark approved
- **Reject**: Analysis is wrong ‚Üí provide feedback
- **Refine**: Need corrections ‚Üí update analysis

#### 4. Feedback Loop

Approved/rejected items feed back into:
- **Training data** for ReAct agent
- **Knowledge base** updates
- **Metrics** for CRAG effectiveness

### HITL API Endpoints

**Get Pending Reviews**:
```bash
GET /api/hitl/pending?priority=high
```

**Approve Review**:
```bash
POST /api/hitl/approve
{
  "id": 123,
  "feedback": "Analysis is correct, TOKEN_EXPIRATION confirmed"
}
```

**Reject Review**:
```bash
POST /api/hitl/reject
{
  "id": 123,
  "feedback": "Root cause is wrong - it's a database issue not auth",
  "correct_analysis": { ... }
}
```

### HITL Metrics

Track these metrics:
- **Queue depth**: Should stay < 50
- **Approval rate**: Typical 80-90%
- **SLA compliance**: > 95%
- **Average review time**: Target < 15 minutes

---

## Web Search Fallback

Web search provides external information when RAG completely fails (confidence < 0.40).

### When Web Search Triggers

- Confidence < 0.40 (VERY_LOW)
- Internal RAG has no relevant information
- Error type likely not in knowledge base
- WebSearchFallback available (graceful degradation if not)

### Web Search Algorithm

```
1. EXTRACT TECHNICAL TERMS
   ‚îú‚îÄ Error types (ValueError, AssertionError, etc.)
   ‚îú‚îÄ File names (test_auth.py, middleware.py, etc.)
   ‚îú‚îÄ Function names (camelCase, snake_case)
   ‚îî‚îÄ Quoted strings (likely important)

2. GENERATE SEARCH QUERY
   ‚îú‚îÄ Error type + technical terms + "fix" + "solution"
   ‚îú‚îÄ Example: "AssertionError TOKEN_EXPIRATION auth middleware fix"
   ‚îî‚îÄ Category-specific keywords added

3. QUERY SEARCH ENGINES (with fallback)
   ‚îú‚îÄ Try Google Custom Search API (if API key configured)
   ‚îú‚îÄ Fallback to Bing Search API (if API key configured)
   ‚îî‚îÄ Fallback to DuckDuckGo (free, no API key)

4. EXTRACT AND RANK SNIPPETS
   ‚îú‚îÄ Extract title + snippet from each result
   ‚îú‚îÄ Clean HTML and format text
   ‚îú‚îÄ Prioritize Stack Overflow, GitHub, official docs
   ‚îî‚îÄ Take top 5 results

5. ENHANCE ANSWER
   ‚îú‚îÄ Prepend "ENHANCED WITH WEB SEARCH:" to answer
   ‚îú‚îÄ Add web-sourced information to fix recommendation
   ‚îú‚îÄ Include source URLs for verification
   ‚îî‚îÄ Combine with original ReAct analysis

6. ESTIMATE NEW CONFIDENCE
   ‚îú‚îÄ Base boost: +0.15 (web search attempted)
   ‚îú‚îÄ Result boost: +0.10 per good result (max 0.30)
   ‚îú‚îÄ Quality boost: +0.20 if high-quality sources
   ‚îî‚îÄ Cap at 0.85 (web results never "HIGH" confidence)
```

### Search Engine Priority

1. **Google Custom Search** (best quality, requires API key)
2. **Bing Search API** (good quality, requires API key)
3. **DuckDuckGo** (free, no API key, acceptable quality)

### Example: Successful Web Search

**Original State**:
```json
{
  "confidence": 0.35,
  "root_cause": "Unknown authentication error",
  "fix_recommendation": "Check configuration"
}
```

**Search Query Generated**: `"authentication error TOKEN_EXPIRATION fix solution"`

**Search Results**:
```json
[
  {
    "url": "https://stackoverflow.com/questions/12345",
    "title": "How to fix TOKEN_EXPIRATION authentication errors",
    "snippet": "Update TOKEN_EXPIRATION to 3600 seconds in middleware.py..."
  },
  {
    "url": "https://github.com/example/issues/789",
    "title": "Auth timeout fix - increase token TTL",
    "snippet": "Solution: Change TOKEN_EXPIRATION constant from 1800 to 3600..."
  }
]
```

**Enhanced Answer**:
```json
{
  "status": "WEB_SEARCH",
  "confidence": 0.62,
  "root_cause": "ENHANCED: Token expiration timeout in authentication middleware (from Stack Overflow #12345)",
  "fix_recommendation": "ENHANCED: Update TOKEN_EXPIRATION from 1800 to 3600 seconds in middleware.py. Verified solution from GitHub issue #789.",
  "web_sources": [
    {"url": "https://stackoverflow.com/questions/12345", "title": "..."},
    {"url": "https://github.com/example/issues/789", "title": "..."}
  ]
}
```

### Web Search Metrics

Track these metrics:
- **Success rate**: Target 50%
- **Average improvement**: Typical 0.15 - 0.25
- **Source quality**: % from Stack Overflow, GitHub, official docs
- **Time cost**: 1-3 seconds per search

### Configuration

**Required Environment Variables** (optional, falls back to DuckDuckGo):

```bash
# Google Custom Search (recommended)
GOOGLE_API_KEY=your_google_api_key
GOOGLE_CSE_ID=your_custom_search_engine_id

# Bing Search API (alternative)
BING_API_KEY=your_bing_api_key
```

---

## Metrics and Monitoring

CRAG includes comprehensive metrics tracking for observability.

### Metrics Tracked

#### Confidence Distribution
- HIGH (‚â•0.85): % of verifications
- MEDIUM (0.65-0.85): % of verifications
- LOW (0.40-0.65): % of verifications
- VERY_LOW (<0.40): % of verifications

**Target**: 60-70% HIGH, 20-30% MEDIUM, 10-15% LOW, 5-10% VERY_LOW

#### Routing Decisions
- PASS: Pass-through count
- HITL: HITL queue count
- CORRECTED: Self-correction successes
- WEB_SEARCH: Web search enhancements

#### Component Scores (Averages)
- Relevance: Average across all verifications
- Consistency: Average across all verifications
- Grounding: Average across all verifications
- Completeness: Average across all verifications
- Classification: Average across all verifications

#### Self-Correction Stats
- Total attempts
- Successes vs. failures
- Success rate (target: 60%)
- Average confidence improvement

#### HITL Queue Stats
- Total queued
- Currently pending
- Approved vs. rejected
- Approval rate
- Priority distribution

#### Web Search Stats
- Total attempts
- Successes vs. failures
- Success rate (target: 50%)
- Average confidence improvement

#### Time-Series Data
- Hourly verification counts
- Daily verification counts
- Trends over time

### API Endpoints

**Get Comprehensive Metrics**:
```bash
GET /api/crag/metrics
```

Response:
```json
{
  "summary": {
    "total_verifications": 1247,
    "uptime_hours": 24.5,
    "verifications_per_hour": 50.9,
    "average_confidence": 0.823
  },
  "confidence_distribution": {
    "HIGH": {"count": 856, "percentage": 68.6},
    "MEDIUM": {"count": 262, "percentage": 21.0},
    "LOW": {"count": 87, "percentage": 7.0},
    "VERY_LOW": {"count": 42, "percentage": 3.4}
  },
  "self_correction": {
    "success_rate": 59.8,
    "average_improvement": 0.152
  },
  ...
}
```

**Get Health Status**:
```bash
GET /api/crag/health
```

Response:
```json
{
  "status": "healthy" | "warning",
  "warnings": [
    "HITL queue large (65 pending)",
    "Low self-correction rate (38.5%)"
  ],
  "metrics_summary": {
    "total_verifications": 1247,
    "hitl_pending": 65,
    "high_confidence_rate": 68.6,
    "self_correction_rate": 38.5
  }
}
```

### Health Monitoring Thresholds

**WARNING Triggered When**:
- HITL queue > 50 pending items
- Self-correction success rate < 40% (after 10+ attempts)
- Web search success rate < 30% (after 10+ attempts)
- High-confidence rate < 30% (after 20+ verifications)

### Dashboard Integration

Recommended visualizations:
- **Confidence distribution pie chart**
- **Routing decisions over time (line chart)**
- **Component scores radar chart**
- **HITL queue depth (line chart with threshold)**
- **Success rates (line charts with target lines)**

---

## Integration Guide

### Basic Integration

```python
from verification.crag_verifier import CRAGVerifier

# Initialize verifier (singleton pattern)
verifier = CRAGVerifier()

# Verify ReAct result
result = verifier.verify(
    react_result={
        'root_cause': 'Token expiration in auth/middleware.py',
        'fix_recommendation': 'Update TOKEN_EXPIRATION to 3600',
        'error_category': 'CODE_ERROR',
        'classification_confidence': 0.85
    },
    retrieved_docs=[
        {'similarity_score': 0.88, 'text': 'TOKEN_EXPIRATION docs...'},
        {'similarity_score': 0.82, 'text': 'Authentication configuration...'}
    ],
    failure_data={
        'build_id': 'BUILD-12345',
        'error_message': 'AssertionError: Expected 200, got 401'
    }
)

# Check result
if result['status'] == 'PASS':
    # High confidence - use immediately
    return result['answer']
elif result['status'] == 'HITL':
    # Medium confidence - show provisional with review link
    return {
        'answer': result['answer'],
        'note': 'Under review',
        'review_url': result['review_url']
    }
elif result['status'] == 'CORRECTED':
    # Self-corrected - use improved answer
    return result['answer']
elif result['status'] == 'WEB_SEARCH':
    # Web-enhanced - use with sources
    return {
        'answer': result['answer'],
        'sources': result['verification_metadata']['web_sources']
    }
```

### Integration with ai_analysis_service.py

```python
# Step 1: Analyze with ReAct
react_result = analyze_with_react_agent(failure_data)

# Step 2: Verify with CRAG
verification_result = verify_react_result_with_crag(react_result, failure_data)

# Step 3: Format with Gemini (using verified result)
result_to_format = verification_result.get('verified_answer', react_result)
formatted_result = format_react_result_with_gemini(result_to_format)

# Add CRAG metadata to response
formatted_result['crag_verified'] = verification_result.get('verified', False)
formatted_result['crag_confidence'] = verification_result.get('confidence', 0.0)
formatted_result['crag_status'] = verification_result.get('verification_status')
```

---

## Usage Examples

### Example 1: High Confidence (Pass Through)

```python
react_result = {
    'root_cause': 'AssertionError in test_auth.py line 45 due to TOKEN_EXPIRATION set to 1800 seconds (30 minutes). Test suite takes > 30 minutes causing token expiration.',
    'fix_recommendation': 'Update TOKEN_EXPIRATION from 1800 to 3600 seconds in auth/middleware.py line 45. Restart service. Run pytest tests/test_auth.py to verify.',
    'error_category': 'CODE_ERROR',
    'classification_confidence': 0.95
}

docs = [
    {'similarity_score': 0.92, 'text': 'TOKEN_EXPIRATION in auth/middleware.py...'},
    {'similarity_score': 0.89, 'text': 'Token expiration configuration...'}
]

result = verifier.verify(react_result, docs, {'build_id': 'TEST-001'})

# Result:
{
    'status': 'PASS',
    'confidence': 0.92,
    'confidence_level': 'HIGH',
    'action_taken': 'pass_through',
    'answer': { /* verified react_result */ }
}
```

### Example 2: Medium Confidence (HITL)

```python
react_result = {
    'root_cause': 'Authentication issue in middleware',
    'fix_recommendation': 'Update token configuration',
    'error_category': 'CODE_ERROR',
    'classification_confidence': 0.70
}

docs = [
    {'similarity_score': 0.68, 'text': 'Token configuration docs...'}
]

result = verifier.verify(react_result, docs, {'build_id': 'TEST-002'})

# Result:
{
    'status': 'HITL',
    'confidence': 0.72,
    'confidence_level': 'MEDIUM',
    'action_taken': 'queued_for_hitl',
    'answer': { /* provisional react_result */ },
    'review_url': '/review/TEST-002',
    'verification_metadata': {
        'queue_id': 123,
        'priority': 'medium',
        'concerns': ['Moderate confidence', 'Incomplete details']
    }
}
```

### Example 3: Low Confidence (Self-Correction)

```python
react_result = {
    'root_cause': 'Unknown authentication error',
    'fix_recommendation': 'Check configuration',
    'error_category': 'UNKNOWN',
    'classification_confidence': 0.50
}

docs = [
    {'similarity_score': 0.52, 'text': 'General auth info...'}
]

result = verifier.verify(react_result, docs, {'build_id': 'TEST-003'})

# Result (if self-correction succeeds):
{
    'status': 'CORRECTED',
    'confidence': 0.68,
    'confidence_level': 'CORRECTED',
    'action_taken': 'self_correction',
    'answer': { /* improved answer with better docs */ },
    'verification_metadata': {
        'original_confidence': 0.55,
        'new_confidence': 0.68,
        'improvement': 0.13
    }
}
```

### Example 4: Very Low Confidence (Web Search)

```python
react_result = {
    'root_cause': 'Unknown',
    'fix_recommendation': 'Unknown',
    'error_category': 'UNKNOWN',
    'classification_confidence': 0.25
}

docs = [
    {'similarity_score': 0.30, 'text': 'Unrelated content...'}
]

result = verifier.verify(react_result, docs, {'build_id': 'TEST-004'})

# Result (if web search succeeds):
{
    'status': 'WEB_SEARCH',
    'confidence': 0.62,
    'confidence_level': 'WEB_ENHANCED',
    'action_taken': 'web_search_fallback',
    'answer': { /* enhanced with web information */ },
    'verification_metadata': {
        'web_sources': [
            {'url': 'https://stackoverflow.com/q/12345', 'title': '...'}
        ],
        'search_engine': 'google'
    }
}
```

---

## Configuration

### Environment Variables

```bash
# PostgreSQL (for HITL queue)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=ddn_qa
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_password

# Google Custom Search (for web search fallback)
GOOGLE_API_KEY=your_google_api_key
GOOGLE_CSE_ID=your_custom_search_engine_id

# Bing Search API (alternative to Google)
BING_API_KEY=your_bing_api_key

# Notification Webhooks (for HITL)
TEAMS_WEBHOOK_URL=https://outlook.office.com/webhook/...
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...
```

### Tuning Thresholds

If needed, you can adjust confidence thresholds:

```python
from verification.crag_verifier import CRAGVerifier

verifier = CRAGVerifier()

# Adjust thresholds (not recommended without testing)
verifier.THRESHOLD_HIGH = 0.90      # Default: 0.85
verifier.THRESHOLD_MEDIUM = 0.70    # Default: 0.65
verifier.THRESHOLD_LOW = 0.45       # Default: 0.40
```

**‚ö†Ô∏è Warning**: Changing thresholds affects routing percentages. Test thoroughly before deployment.

### Component Weights

Adjust scoring weights if needed:

```python
from verification.crag_verifier import ConfidenceScorer

scorer = ConfidenceScorer()

# Adjust weights (must sum to 1.0)
scorer.weights = {
    'relevance': 0.30,      # Increase relevance importance
    'consistency': 0.15,    # Decrease consistency importance
    'grounding': 0.25,      # Keep grounding high
    'completeness': 0.15,   # Keep completeness
    'classification': 0.15  # Keep classification
}
```

---

## Troubleshooting

### Issue: All answers routing to HITL

**Symptoms**: HITL queue growing rapidly, few PASS answers

**Possible Causes**:
1. Poor RAG retrieval quality (low similarity scores)
2. Incomplete ReAct answers
3. Thresholds too strict

**Solutions**:
1. Check Pinecone vector quality:
   ```python
   knowledge_stats = knowledge_index.describe_index_stats()
   print(f"Total vectors: {knowledge_stats.total_vector_count}")
   ```

2. Review component scores in metrics:
   ```bash
   GET /api/crag/metrics
   # Look at 'component_scores' - which is lowest?
   ```

3. Temporarily lower HIGH threshold for testing:
   ```python
   verifier.THRESHOLD_HIGH = 0.80  # Test only!
   ```

---

### Issue: Self-correction not improving confidence

**Symptoms**: Self-correction success rate < 40%

**Possible Causes**:
1. Query expansion not finding better docs
2. Pinecone indexes incomplete
3. Error categories not well-represented

**Solutions**:
1. Review self-correction queries:
   ```python
   # Enable debug logging
   import logging
   logging.basicConfig(level=logging.DEBUG)
   ```

2. Add more error documentation to Pinecone

3. Review failed self-corrections:
   ```python
   metrics = get_metrics()
   stats = metrics.get_statistics()
   print(f"Self-correction rate: {stats['self_correction']['success_rate']}")
   ```

---

### Issue: Web search always failing

**Symptoms**: Web search success rate < 30%

**Possible Causes**:
1. No API keys configured (using DuckDuckGo only)
2. Search queries not generating good results
3. Rate limiting

**Solutions**:
1. Configure Google or Bing API:
   ```bash
   export GOOGLE_API_KEY=your_key
   export GOOGLE_CSE_ID=your_cse_id
   ```

2. Review generated search queries:
   ```python
   # Enable debug logging
   logging.getLogger('web_search_fallback').setLevel(logging.DEBUG)
   ```

3. Check rate limits and add delays if needed

---

### Issue: HITL queue backing up

**Symptoms**: Queue depth > 50, SLA violations

**Possible Causes**:
1. Not enough reviewers
2. Review process too slow
3. Too many MEDIUM confidence answers

**Solutions**:
1. Add more reviewers to team

2. Prioritize high-priority reviews:
   ```bash
   GET /api/hitl/pending?priority=high&limit=20
   ```

3. Consider lowering MEDIUM threshold:
   ```python
   verifier.THRESHOLD_MEDIUM = 0.60  # More answers pass through
   ```

---

## Performance Characteristics

### Latency

| Operation | Latency | Notes |
|-----------|---------|-------|
| HIGH confidence (pass-through) | < 50ms | Just scoring |
| MEDIUM confidence (HITL queue) | < 100ms | Scoring + DB insert |
| LOW confidence (self-correction) | 500ms - 2s | Re-retrieval + re-analysis |
| VERY_LOW confidence (web search) | 1-3s | External API call |

**Overall**: 95th percentile < 500ms for non-corrective paths

### Throughput

- **Without correction**: 200+ verifications/second
- **With 10% correction**: 150+ verifications/second
- **With 5% web search**: 100+ verifications/second

### Resource Usage

- **Memory**: ~10MB for verifier + 100KB per 1000 verifications (metrics)
- **CPU**: Minimal (scoring is fast)
- **Network**: Only for web search (1-3s per search)
- **Database**: HITL queue inserts only (< 100ms)

---

## Conclusion

CRAG provides a robust verification layer that:

‚úÖ **Quantifies uncertainty** through multi-dimensional scoring
‚úÖ **Takes corrective action** based on confidence levels
‚úÖ **Learns from feedback** through HITL and metrics
‚úÖ **Degrades gracefully** when components unavailable
‚úÖ **Monitors performance** with comprehensive metrics

### Key Takeaways

1. **Trust the thresholds**: They're calibrated for 60-70% pass-through
2. **Monitor metrics**: Watch for degradation in component scores
3. **Keep HITL queue small**: < 50 pending items
4. **Target self-correction success**: 60%
5. **Target web search success**: 50%

### Next Steps

- **Phase 0-ARCH.22**: Performance test CRAG with 50 diverse errors
- **Phase 0-ARCH.23**: Design Fusion RAG architecture
- **Phase 1**: Add caching and optimization

---

**Documentation Version**: 1.4.0
**Last Updated**: 2025-11-02
**Maintained By**: AI Analysis System Team
**Feedback**: Report issues at [GitHub Issues](https://github.com/your-repo/issues)
