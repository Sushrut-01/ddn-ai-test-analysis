{
  "version": "1.0.0",
  "last_updated": "2025-10-24",
  "description": "DDN Storage Test Error Documentation Database for RAG Integration",
  "errors": [
    {
      "error_id": "ERR001",
      "error_type": "NullPointerException",
      "error_category": "CODE",
      "subcategory": "Null Pointer Access",
      "error_message": "java.lang.NullPointerException: Cannot invoke \"DDNStorage.saveDataBindFile\" because \"this.storageConfig\" is null",
      "component": "DDN Storage Configuration",
      "file_path": "src/main/java/com/ddn/storage/DDNStorage.java",
      "line_range": "125-135",
      "root_cause": "The storageConfig object is accessed without null validation. When DDN storage initialization fails or is skipped, attempting to call methods on the null storageConfig object causes NullPointerException.",
      "code_before": "public class DDNStorage {\n    private StorageConfig storageConfig;\n    \n    public void saveData(String filePath, byte[] data) {\n        // Direct access without null check\n        storageConfig.saveDataBindFile(filePath, data);\n    }\n}",
      "code_after": "public class DDNStorage {\n    private StorageConfig storageConfig;\n    \n    public void saveData(String filePath, byte[] data) {\n        // Added null check with clear error message\n        if (storageConfig == null) {\n            throw new IllegalStateException(\n                \"DDN Storage not initialized. Call init() before saveData().\"\n            );\n        }\n        storageConfig.saveDataBindFile(filePath, data);\n    }\n}",
      "solution_steps": [
        "Add null check before accessing storageConfig object",
        "Throw IllegalStateException with descriptive message",
        "Guide developer to call init() method first",
        "Prevents NPE and provides actionable error context",
        "Add @NonNull annotation to enforce initialization"
      ],
      "prevention": "Always validate object state before method calls. Use initialization flags or builder patterns.",
      "severity": "HIGH",
      "frequency": "Common in initialization flows",
      "related_errors": ["ERR002", "ERR015"],
      "test_scenarios": ["EXAScaler storage initialization", "AI400X checkpoint storage"],
      "tags": ["null-pointer", "initialization", "storage-config", "validation"]
    },
    {
      "error_id": "ERR002",
      "error_type": "ConnectionRefusedException",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "Network Connectivity",
      "error_message": "java.net.ConnectException: Connection refused: connect to http://exascaler.ddn.local:8080",
      "component": "EXAScaler Lustre Client",
      "file_path": "src/main/java/com/ddn/exascaler/LustreClient.java",
      "line_range": "45-52",
      "root_cause": "EXAScaler endpoint is not reachable. Common causes: service not running, incorrect hostname/port, firewall blocking connection, or DNS resolution failure.",
      "code_before": "public class LustreClient {\n    private String endpoint = \"http://exascaler.ddn.local:8080\";\n    \n    public Response getHealth() {\n        // No retry or timeout configuration\n        HttpClient client = HttpClient.newHttpClient();\n        HttpRequest request = HttpRequest.newBuilder()\n            .uri(URI.create(endpoint + \"/api/v1/health\"))\n            .build();\n        return client.send(request, HttpResponse.BodyHandlers.ofString());\n    }\n}",
      "code_after": "public class LustreClient {\n    private String endpoint = \"http://exascaler.ddn.local:8080\";\n    private static final int MAX_RETRIES = 3;\n    private static final int TIMEOUT_SECONDS = 10;\n    \n    public Response getHealth() {\n        HttpClient client = HttpClient.newBuilder()\n            .connectTimeout(Duration.ofSeconds(TIMEOUT_SECONDS))\n            .build();\n        \n        for (int retry = 0; retry < MAX_RETRIES; retry++) {\n            try {\n                HttpRequest request = HttpRequest.newBuilder()\n                    .uri(URI.create(endpoint + \"/api/v1/health\"))\n                    .timeout(Duration.ofSeconds(TIMEOUT_SECONDS))\n                    .build();\n                return client.send(request, HttpResponse.BodyHandlers.ofString());\n            } catch (ConnectException e) {\n                if (retry == MAX_RETRIES - 1) {\n                    throw new DDNConnectionException(\n                        \"Failed to connect to EXAScaler at \" + endpoint + \n                        \" after \" + MAX_RETRIES + \" attempts. \" +\n                        \"Verify service is running and network is accessible.\",\n                        e\n                    );\n                }\n                Thread.sleep(1000 * (retry + 1)); // Exponential backoff\n            }\n        }\n    }\n}",
      "solution_steps": [
        "Verify EXAScaler service is running: systemctl status exascaler",
        "Check network connectivity: ping exascaler.ddn.local",
        "Verify DNS resolution: nslookup exascaler.ddn.local",
        "Check firewall rules allow port 8080",
        "Add retry logic with exponential backoff",
        "Configure appropriate timeouts (10-30 seconds)",
        "Provide clear error message with troubleshooting steps"
      ],
      "prevention": "Always implement retry logic for network calls. Use circuit breaker pattern for fault tolerance.",
      "severity": "CRITICAL",
      "frequency": "Common in distributed environments",
      "related_errors": ["ERR003", "ERR004", "ERR010"],
      "test_scenarios": ["EXAScaler health check", "Cluster status verification"],
      "tags": ["connection-refused", "network", "exascaler", "retry", "timeout"]
    },
    {
      "error_id": "ERR003",
      "error_type": "DNSResolutionException",
      "error_category": "ENVIRONMENT",
      "subcategory": "DNS Configuration",
      "error_message": "java.net.UnknownHostException: exascaler.ddn.local: Name or service not known",
      "component": "Network Configuration",
      "file_path": "src/main/resources/application.properties",
      "line_range": "12-15",
      "root_cause": "DNS cannot resolve the DDN storage hostname. Either DNS server is not configured, hostname is incorrect, or /etc/hosts file is missing entry.",
      "code_before": "# application.properties\nddn.exascaler.endpoint=http://exascaler.ddn.local:8080\nddn.ai400x.endpoint=http://ai400x.ddn.local:8080\n\n# No fallback or validation",
      "code_after": "# application.properties\nddn.exascaler.endpoint=http://exascaler.ddn.local:8080\nddn.exascaler.fallback.ip=10.10.1.50\nddn.ai400x.endpoint=http://ai400x.ddn.local:8080\nddn.ai400x.fallback.ip=10.10.1.51\n\n# DNS validation on startup\nddn.dns.validation.enabled=true\nddn.dns.validation.failfast=false",
      "solution_steps": [
        "Check DNS configuration: cat /etc/resolv.conf",
        "Test DNS resolution: nslookup exascaler.ddn.local",
        "Add entry to /etc/hosts if DNS unavailable:",
        "  10.10.1.50    exascaler.ddn.local",
        "  10.10.1.51    ai400x.ddn.local",
        "Verify network interface has correct DNS servers",
        "Use IP address as fallback in configuration",
        "Implement DNS validation on application startup"
      ],
      "prevention": "Always configure fallback IPs. Validate DNS resolution during initialization.",
      "severity": "HIGH",
      "frequency": "Common in new deployments",
      "related_errors": ["ERR002", "ERR010"],
      "test_scenarios": ["All DDN connectivity tests", "Initial deployment"],
      "tags": ["dns", "hostname", "network", "configuration"]
    },
    {
      "error_id": "ERR004",
      "error_type": "AuthenticationException",
      "error_category": "CONFIGURATION",
      "subcategory": "API Credentials",
      "error_message": "401 Unauthorized: Invalid API key or secret for DDN EXAScaler API",
      "component": "API Authentication",
      "file_path": "src/main/java/com/ddn/auth/ApiAuthenticator.java",
      "line_range": "30-40",
      "root_cause": "API key or secret is incorrect, expired, or not properly configured in environment variables. DDN APIs require valid credentials for all requests.",
      "code_before": "public class ApiAuthenticator {\n    public String getAuthHeader() {\n        // Hardcoded credentials (BAD PRACTICE)\n        String apiKey = \"old_api_key_123\";\n        String apiSecret = \"old_secret_456\";\n        \n        String credentials = apiKey + \":\" + apiSecret;\n        String encoded = Base64.getEncoder().encodeToString(credentials.getBytes());\n        return \"Basic \" + encoded;\n    }\n}",
      "code_after": "public class ApiAuthenticator {\n    private final String apiKey;\n    private final String apiSecret;\n    \n    public ApiAuthenticator() {\n        // Load from environment variables\n        this.apiKey = System.getenv(\"DDN_API_KEY\");\n        this.apiSecret = System.getenv(\"DDN_API_SECRET\");\n        \n        // Validate credentials are present\n        if (apiKey == null || apiKey.isEmpty()) {\n            throw new IllegalStateException(\n                \"DDN_API_KEY environment variable not set. \" +\n                \"Set it before running tests.\"\n            );\n        }\n        if (apiSecret == null || apiSecret.isEmpty()) {\n            throw new IllegalStateException(\n                \"DDN_API_SECRET environment variable not set. \" +\n                \"Set it before running tests.\"\n            );\n        }\n    }\n    \n    public String getAuthHeader() {\n        String credentials = apiKey + \":\" + apiSecret;\n        String encoded = Base64.getEncoder().encodeToString(credentials.getBytes());\n        return \"Basic \" + encoded;\n    }\n}",
      "solution_steps": [
        "Verify DDN_API_KEY environment variable is set",
        "Verify DDN_API_SECRET environment variable is set",
        "Check API key is not expired in DDN EMF console",
        "Regenerate API key if necessary",
        "Update Jenkins credentials with new key",
        "Never hardcode credentials in source code",
        "Use environment variables or secrets manager",
        "Add validation during initialization"
      ],
      "prevention": "Always use environment variables for credentials. Validate on startup.",
      "severity": "CRITICAL",
      "frequency": "Common during initial setup",
      "related_errors": ["ERR005", "ERR014"],
      "test_scenarios": ["All API-based tests", "Authentication flows"],
      "tags": ["authentication", "api-key", "credentials", "401", "security"]
    },
    {
      "error_id": "ERR005",
      "error_type": "S3AccessDeniedException",
      "error_category": "CONFIGURATION",
      "subcategory": "S3 Permissions",
      "error_message": "com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403)",
      "component": "S3 Protocol Access",
      "file_path": "src/main/java/com/ddn/s3/S3ClientManager.java",
      "line_range": "55-70",
      "root_cause": "S3 access key lacks permissions for bucket operations, or bucket policy denies access. Could also be incorrect bucket name or cross-tenant access attempt.",
      "code_before": "public class S3ClientManager {\n    public void createBucket(String bucketName) {\n        AmazonS3 s3Client = AmazonS3ClientBuilder.standard()\n            .withCredentials(new AWSStaticCredentialsProvider(\n                new BasicAWSCredentials(accessKey, secretKey)\n            ))\n            .withEndpointConfiguration(\n                new EndpointConfiguration(s3Endpoint, \"us-east-1\")\n            )\n            .build();\n        \n        // No error handling for permissions\n        s3Client.createBucket(bucketName);\n    }\n}",
      "code_after": "public class S3ClientManager {\n    public void createBucket(String bucketName) {\n        AmazonS3 s3Client = AmazonS3ClientBuilder.standard()\n            .withCredentials(new AWSStaticCredentialsProvider(\n                new BasicAWSCredentials(accessKey, secretKey)\n            ))\n            .withEndpointConfiguration(\n                new EndpointConfiguration(s3Endpoint, \"us-east-1\")\n            )\n            .build();\n        \n        try {\n            // Check if bucket already exists\n            if (s3Client.doesBucketExistV2(bucketName)) {\n                logger.info(\"Bucket {} already exists\", bucketName);\n                return;\n            }\n            \n            s3Client.createBucket(bucketName);\n            logger.info(\"Successfully created bucket {}\", bucketName);\n            \n        } catch (AmazonS3Exception e) {\n            if (e.getStatusCode() == 403) {\n                throw new DDNAccessDeniedException(\n                    \"Access denied for bucket '\" + bucketName + \"'. \" +\n                    \"Verify S3 credentials have s3:CreateBucket permission. \" +\n                    \"Check bucket policy and IAM roles.\",\n                    e\n                );\n            } else if (e.getStatusCode() == 409) {\n                throw new DDNBucketConflictException(\n                    \"Bucket '\" + bucketName + \"' already exists in different tenant.\",\n                    e\n                );\n            }\n            throw e;\n        }\n    }\n}",
      "solution_steps": [
        "Verify S3 access key has required permissions:",
        "  - s3:CreateBucket",
        "  - s3:ListBucket",
        "  - s3:PutObject",
        "  - s3:GetObject",
        "Check bucket policy in DDN EMF console",
        "Verify tenant isolation is not blocking access",
        "Ensure bucket name follows naming conventions",
        "Test with AWS CLI: aws s3 ls --endpoint-url http://s3.exascaler.ddn.local",
        "Review IAM roles if using role-based access"
      ],
      "prevention": "Validate S3 permissions before operations. Use least-privilege principle.",
      "severity": "HIGH",
      "frequency": "Common in multi-tenancy setups",
      "related_errors": ["ERR004", "ERR016"],
      "test_scenarios": ["S3 multi-tenancy tests", "Bucket creation"],
      "tags": ["s3", "access-denied", "permissions", "403", "multi-tenancy"]
    },
    {
      "error_id": "ERR006",
      "error_type": "QuotaExceededException",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "Storage Quota",
      "error_message": "Quota exceeded: Tenant has reached hard limit of 1000 GB",
      "component": "Storage Quota Management",
      "file_path": "src/main/java/com/ddn/quota/QuotaManager.java",
      "line_range": "80-95",
      "root_cause": "Tenant has consumed all allocated storage quota. Hard limit reached prevents further writes. Soft limit warning was likely ignored earlier.",
      "code_before": "public class QuotaManager {\n    public void writeData(String namespace, byte[] data) {\n        // No quota check before write\n        storageService.write(namespace, data);\n    }\n}",
      "code_after": "public class QuotaManager {\n    public void writeData(String namespace, byte[] data) {\n        // Check quota before write\n        QuotaInfo quota = getQuotaUsage(namespace);\n        \n        long dataSize = data.length;\n        long availableSpace = quota.getHardLimitBytes() - quota.getUsedBytes();\n        \n        if (dataSize > availableSpace) {\n            throw new QuotaExceededException(\n                String.format(\n                    \"Quota exceeded for namespace '%s'. \" +\n                    \"Attempting to write %d MB but only %d MB available. \" +\n                    \"Hard limit: %d GB, Used: %d GB. \" +\n                    \"Contact admin to increase quota.\",\n                    namespace,\n                    dataSize / (1024 * 1024),\n                    availableSpace / (1024 * 1024),\n                    quota.getHardLimitGB(),\n                    quota.getUsedGB()\n                )\n            );\n        }\n        \n        // Warn if approaching soft limit (90%)\n        if (quota.getUsagePercentage() > 90.0) {\n            logger.warn(\n                \"Namespace '{}' is at {}% quota (soft limit exceeded)\",\n                namespace, quota.getUsagePercentage()\n            );\n        }\n        \n        storageService.write(namespace, data);\n    }\n}",
      "solution_steps": [
        "Check current quota usage: lfs quota -h /lustre/namespace",
        "Identify large files consuming quota",
        "Clean up unnecessary data or old checkpoints",
        "Request quota increase from administrator",
        "Update quota in DDN EMF console",
        "Implement data lifecycle management",
        "Add monitoring alerts at 80% and 90% thresholds",
        "Archive old data to cheaper storage tier"
      ],
      "prevention": "Monitor quota usage proactively. Implement automated cleanup policies.",
      "severity": "MEDIUM",
      "frequency": "Common in multi-tenant environments",
      "related_errors": ["ERR017"],
      "test_scenarios": ["Quota management tests", "Storage capacity tests"],
      "tags": ["quota", "storage", "capacity", "multi-tenancy", "limit"]
    },
    {
      "error_id": "ERR007",
      "error_type": "CheckpointCorruptedException",
      "error_category": "DATA",
      "subcategory": "Data Integrity",
      "error_message": "AI model checkpoint corrupted: Checksum mismatch for checkpoint_epoch_100.pt",
      "component": "AI400X Checkpoint Storage",
      "file_path": "src/main/java/com/ddn/ai400x/CheckpointManager.java",
      "line_range": "120-140",
      "root_cause": "Checkpoint file was corrupted during storage or retrieval. Could be due to incomplete write, network error during transfer, or hardware failure.",
      "code_before": "public class CheckpointManager {\n    public void saveCheckpoint(String modelName, byte[] checkpointData) {\n        String path = \"/checkpoints/\" + modelName + \".pt\";\n        // No checksum or verification\n        fileSystem.write(path, checkpointData);\n    }\n    \n    public byte[] loadCheckpoint(String modelName) {\n        String path = \"/checkpoints/\" + modelName + \".pt\";\n        // No integrity check\n        return fileSystem.read(path);\n    }\n}",
      "code_after": "public class CheckpointManager {\n    public void saveCheckpoint(String modelName, byte[] checkpointData) {\n        String path = \"/checkpoints/\" + modelName + \".pt\";\n        \n        // Calculate checksum before write\n        String checksum = calculateSHA256(checkpointData);\n        \n        // Write checkpoint data\n        fileSystem.write(path, checkpointData);\n        \n        // Write checksum file\n        String checksumPath = path + \".sha256\";\n        fileSystem.write(checksumPath, checksum.getBytes());\n        \n        logger.info(\"Saved checkpoint {} with checksum {}\", modelName, checksum);\n    }\n    \n    public byte[] loadCheckpoint(String modelName) {\n        String path = \"/checkpoints/\" + modelName + \".pt\";\n        String checksumPath = path + \".sha256\";\n        \n        // Read checkpoint data\n        byte[] checkpointData = fileSystem.read(path);\n        \n        // Read expected checksum\n        String expectedChecksum = new String(fileSystem.read(checksumPath));\n        \n        // Calculate actual checksum\n        String actualChecksum = calculateSHA256(checkpointData);\n        \n        // Verify integrity\n        if (!expectedChecksum.equals(actualChecksum)) {\n            throw new CheckpointCorruptedException(\n                String.format(\n                    \"Checkpoint '%s' is corrupted. \" +\n                    \"Expected checksum: %s, Actual: %s. \" +\n                    \"The checkpoint must be restored from backup.\",\n                    modelName, expectedChecksum, actualChecksum\n                )\n            );\n        }\n        \n        logger.info(\"Checkpoint {} integrity verified\", modelName);\n        return checkpointData;\n    }\n    \n    private String calculateSHA256(byte[] data) {\n        MessageDigest digest = MessageDigest.getInstance(\"SHA-256\");\n        byte[] hash = digest.digest(data);\n        return Base64.getEncoder().encodeToString(hash);\n    }\n}",
      "solution_steps": [
        "Check if backup checkpoint exists",
        "Restore from last known good checkpoint",
        "Re-run training from previous valid epoch",
        "Verify storage hardware health",
        "Check network stability during transfers",
        "Implement checksums for all checkpoint operations",
        "Use atomic write operations",
        "Enable versioning for checkpoint files"
      ],
      "prevention": "Always use checksums for large data files. Implement backup strategies.",
      "severity": "CRITICAL",
      "frequency": "Rare but high impact",
      "related_errors": ["ERR008", "ERR012"],
      "test_scenarios": ["AI400X checkpoint tests", "Data integrity tests"],
      "tags": ["checkpoint", "corruption", "data-integrity", "ai400x", "hash"]
    },
    {
      "error_id": "ERR008",
      "error_type": "LustreStripingException",
      "error_category": "CONFIGURATION",
      "subcategory": "File Striping",
      "error_message": "Invalid Lustre striping configuration: stripe_count=0 is not allowed",
      "component": "EXAScaler Lustre",
      "file_path": "src/main/java/com/ddn/exascaler/StripingManager.java",
      "line_range": "45-60",
      "root_cause": "Lustre striping parameters are invalid. Stripe count must be >= 1, and stripe size must be power of 2. Incorrect configuration prevents file creation.",
      "code_before": "public class StripingManager {\n    public void createStripedFile(String path, int stripeCount, String stripeSize) {\n        // No validation of parameters\n        Map<String, Object> params = new HashMap<>();\n        params.put(\"path\", path);\n        params.put(\"stripe_count\", stripeCount);\n        params.put(\"stripe_size\", stripeSize);\n        \n        apiClient.post(\"/api/v1/files/create\", params);\n    }\n}",
      "code_after": "public class StripingManager {\n    private static final int MIN_STRIPE_COUNT = 1;\n    private static final int MAX_STRIPE_COUNT = 2000;\n    private static final List<String> VALID_STRIPE_SIZES = \n        Arrays.asList(\"64K\", \"128K\", \"256K\", \"512K\", \"1M\", \"2M\", \"4M\", \"8M\", \"16M\");\n    \n    public void createStripedFile(String path, int stripeCount, String stripeSize) {\n        // Validate stripe count\n        if (stripeCount < MIN_STRIPE_COUNT || stripeCount > MAX_STRIPE_COUNT) {\n            throw new IllegalArgumentException(\n                String.format(\n                    \"Invalid stripe_count=%d. Must be between %d and %d. \" +\n                    \"For large files, use higher stripe count (4-16) for better parallelism.\",\n                    stripeCount, MIN_STRIPE_COUNT, MAX_STRIPE_COUNT\n                )\n            );\n        }\n        \n        // Validate stripe size\n        if (!VALID_STRIPE_SIZES.contains(stripeSize.toUpperCase())) {\n            throw new IllegalArgumentException(\n                String.format(\n                    \"Invalid stripe_size='%s'. Must be one of: %s. \" +\n                    \"Recommended: 1M for general use, 4M+ for large files.\",\n                    stripeSize, String.join(\", \", VALID_STRIPE_SIZES)\n                )\n            );\n        }\n        \n        Map<String, Object> params = new HashMap<>();\n        params.put(\"path\", path);\n        params.put(\"stripe_count\", stripeCount);\n        params.put(\"stripe_size\", stripeSize.toUpperCase());\n        \n        logger.info(\n            \"Creating Lustre striped file: {} (stripe_count={}, stripe_size={})\",\n            path, stripeCount, stripeSize\n        );\n        \n        apiClient.post(\"/api/v1/files/create\", params);\n    }\n}",
      "solution_steps": [
        "Use valid stripe count: 1 to 2000 (typical: 4-16)",
        "Use valid stripe size: 64K, 128K, 256K, 512K, 1M, 2M, 4M, 8M, 16M",
        "For small files (<100MB): stripe_count=1, stripe_size=1M",
        "For large files (>1GB): stripe_count=8-16, stripe_size=4M-16M",
        "Check OSS count: lfs df -h",
        "Don't exceed number of available OSS servers",
        "Use lfs getstripe to verify existing file striping"
      ],
      "prevention": "Validate striping parameters. Use striping best practices based on file size.",
      "severity": "MEDIUM",
      "frequency": "Common during initial setup",
      "related_errors": ["ERR009"],
      "test_scenarios": ["EXAScaler file operations", "Striping tests"],
      "tags": ["lustre", "striping", "exascaler", "configuration", "validation"]
    },
    {
      "error_id": "ERR009",
      "error_type": "InsufficientOSSException",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "Cluster Resources",
      "error_message": "Insufficient OSS servers: Requested stripe_count=16 but only 8 OSS available",
      "component": "EXAScaler Cluster",
      "file_path": "src/main/java/com/ddn/exascaler/ClusterValidator.java",
      "line_range": "70-85",
      "root_cause": "Lustre striping requests more OSS (Object Storage Servers) than are available in cluster. Cannot stripe data across more servers than exist.",
      "code_before": "public class ClusterValidator {\n    public void validateStriping(int requestedStripeCount) {\n        // No validation against available OSS\n        // Proceeding without checking cluster resources\n    }\n}",
      "code_after": "public class ClusterValidator {\n    public void validateStriping(int requestedStripeCount) {\n        // Get cluster status\n        ClusterStatus status = getClusterStatus();\n        int availableOSS = status.getActiveOSSCount();\n        \n        if (requestedStripeCount > availableOSS) {\n            logger.warn(\n                \"Requested stripe_count={} exceeds available OSS={}. \" +\n                \"Adjusting to maximum available.\",\n                requestedStripeCount, availableOSS\n            );\n            \n            throw new InsufficientResourcesException(\n                String.format(\n                    \"Cannot stripe across %d OSS servers - only %d available. \" +\n                    \"Either reduce stripe_count to %d or add more OSS to cluster. \" +\n                    \"Current cluster: %d MDS, %d OSS.\",\n                    requestedStripeCount,\n                    availableOSS,\n                    availableOSS,\n                    status.getActiveMDSCount(),\n                    availableOSS\n                )\n            );\n        }\n        \n        logger.info(\n            \"Striping validation passed: stripe_count={}, available_OSS={}\",\n            requestedStripeCount, availableOSS\n        );\n    }\n    \n    private ClusterStatus getClusterStatus() {\n        Response response = apiClient.get(\"/api/v1/cluster/status\");\n        return response.body(ClusterStatus.class);\n    }\n}",
      "solution_steps": [
        "Check available OSS: lfs df -h",
        "Reduce stripe_count to match available OSS",
        "Or add more OSS servers to cluster",
        "Typical configurations:",
        "  - Small cluster: 4-8 OSS",
        "  - Medium cluster: 16-32 OSS",
        "  - Large cluster: 64+ OSS",
        "Balance stripe count with workload parallelism",
        "Monitor OSS load distribution"
      ],
      "prevention": "Always validate cluster capacity before operations. Auto-adjust striping.",
      "severity": "MEDIUM",
      "frequency": "Common in small clusters",
      "related_errors": ["ERR008", "ERR011"],
      "test_scenarios": ["EXAScaler cluster status", "Resource validation"],
      "tags": ["lustre", "oss", "cluster", "resources", "capacity"]
    },
    {
      "error_id": "ERR010",
      "error_type": "TimeoutException",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "Performance",
      "error_message": "Request timeout: EXAScaler health check took longer than 30 seconds",
      "component": "Health Check",
      "file_path": "src/main/java/com/ddn/monitoring/HealthChecker.java",
      "line_range": "25-40",
      "root_cause": "Storage system is under heavy load or experiencing performance issues. Request exceeded configured timeout threshold. Could indicate disk I/O bottleneck or network congestion.",
      "code_before": "public class HealthChecker {\n    public void checkHealth() {\n        // No timeout configured - waits indefinitely\n        HttpClient client = HttpClient.newHttpClient();\n        HttpRequest request = HttpRequest.newBuilder()\n            .uri(URI.create(endpoint + \"/health\"))\n            .build();\n        \n        client.send(request, HttpResponse.BodyHandlers.ofString());\n    }\n}",
      "code_after": "public class HealthChecker {\n    private static final int HEALTH_CHECK_TIMEOUT_SEC = 30;\n    private static final int CRITICAL_TIMEOUT_SEC = 60;\n    \n    public HealthStatus checkHealth() {\n        HttpClient client = HttpClient.newBuilder()\n            .connectTimeout(Duration.ofSeconds(HEALTH_CHECK_TIMEOUT_SEC))\n            .build();\n        \n        HttpRequest request = HttpRequest.newBuilder()\n            .uri(URI.create(endpoint + \"/health\"))\n            .timeout(Duration.ofSeconds(HEALTH_CHECK_TIMEOUT_SEC))\n            .build();\n        \n        long startTime = System.currentTimeMillis();\n        \n        try {\n            HttpResponse<String> response = client.send(\n                request, \n                HttpResponse.BodyHandlers.ofString()\n            );\n            \n            long duration = System.currentTimeMillis() - startTime;\n            \n            // Check response time thresholds\n            if (duration > CRITICAL_TIMEOUT_SEC * 1000) {\n                return HealthStatus.CRITICAL;\n            } else if (duration > HEALTH_CHECK_TIMEOUT_SEC * 1000) {\n                return HealthStatus.DEGRADED;\n            } else if (duration > 10000) {\n                return HealthStatus.SLOW;\n            }\n            \n            return HealthStatus.HEALTHY;\n            \n        } catch (TimeoutException e) {\n            logger.error(\n                \"Health check timeout after {}s. System may be overloaded. \" +\n                \"Check: 1) OSS/MDS load, 2) Network latency, 3) Disk I/O\",\n                HEALTH_CHECK_TIMEOUT_SEC\n            );\n            return HealthStatus.TIMEOUT;\n        }\n    }\n}",
      "solution_steps": [
        "Check system load: top, htop",
        "Check disk I/O: iostat -x 1",
        "Check network latency: ping -c 10 exascaler.ddn.local",
        "Review OSS/MDS server load in DDN EMF",
        "Check for disk failures or degraded RAID",
        "Investigate slow queries or large operations",
        "Increase timeout for health checks if normal",
        "Consider load balancing if consistently slow",
        "Review recent workload changes"
      ],
      "prevention": "Set appropriate timeouts. Monitor system performance proactively.",
      "severity": "HIGH",
      "frequency": "Varies with load",
      "related_errors": ["ERR002", "ERR011"],
      "test_scenarios": ["Health check tests", "Performance tests"],
      "tags": ["timeout", "performance", "health-check", "latency"]
    }
  ]
}
