{
  "version": "2.0.0",
  "last_updated": "2025-10-31",
  "description": "DDN Storage Test Error Documentation - Phase 2 (Mined from Test Scenarios)",
  "source": "Extracted from ddn-test-scenarios.js and ddn-advanced-scenarios.js",
  "errors": [
    {
      "error_id": "ERR011",
      "error_type": "CrossTenantAccessViolation",
      "error_category": "SECURITY",
      "subcategory": "Multi-Tenancy Isolation",
      "error_message": "HTTP 403 Forbidden: Cross-tenant access denied",
      "component": "Domain Isolation",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "145-182",
      "root_cause": "Tenant attempted to access another tenant's domain or namespace. Domain isolation is correctly preventing unauthorized cross-tenant access. This is EXPECTED behavior for security.",
      "code_before": "// No access control check\nconst data = await fetchTenantData(tenant2Domain);\n// Using Tenant 1 credentials to access Tenant 2 data",
      "code_after": "// Verify tenant authorization before access\nif (currentTenant.domain !== requestedDomain) {\n    throw new ForbiddenError(\n        `Access denied: Tenant ${currentTenant.domain} cannot access ${requestedDomain}`\n    );\n}\nconst data = await fetchTenantData(currentTenant.domain);",
      "solution_steps": [
        "Verify this is NOT a legitimate access request",
        "Confirm tenant credentials match the requested domain",
        "Check that application is using correct tenant context",
        "Review tenant routing configuration",
        "This error indicates security is WORKING - do not disable",
        "If legitimate, request admin to grant cross-tenant permissions",
        "Audit access logs for security compliance"
      ],
      "prevention": "Always validate tenant context before data access. Use middleware for automatic tenant isolation.",
      "severity": "CRITICAL",
      "frequency": "Common in multi-tenant testing",
      "related_errors": ["ERR012", "ERR013", "ERR016"],
      "test_scenarios": ["Domain isolation tests", "Cross-tenant access prevention"],
      "tags": ["security", "multi-tenancy", "403", "access-control", "domain-isolation"]
    },
    {
      "error_id": "ERR012",
      "error_type": "NamespaceAccessViolation",
      "error_category": "SECURITY",
      "subcategory": "Namespace Isolation",
      "error_message": "HTTP 403 Forbidden: Namespace access denied",
      "component": "Lustre Namespace Management",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "330-367",
      "root_cause": "Client attempted to access a namespace (fileset) that belongs to a different tenant. Nodemap configuration correctly restricts access based on client NID and tenant mapping.",
      "code_before": "// Direct namespace access without validation\nconst files = await lustreClient.listFiles('/lustre/tenant2_namespace');\n// No check if current tenant owns this namespace",
      "code_after": "// Validate namespace ownership\nconst namespace = await getNamespace('tenant2_namespace');\nif (namespace.ownerDomain !== currentTenant.domain) {\n    throw new ForbiddenError(\n        `Namespace ${namespace.name} belongs to ${namespace.ownerDomain}, ` +\n        `not ${currentTenant.domain}`\n    );\n}\nconst files = await lustreClient.listFiles(namespace.rootPath);",
      "solution_steps": [
        "Verify client NID matches tenant's network segment",
        "Check nodemap configuration: lctl nodemap_info",
        "Confirm namespace ownership in EMF console",
        "Review fileset-to-tenant mapping",
        "Ensure client is mounting correct namespace path",
        "Check that root squashing is enabled",
        "This is EXPECTED - indicates namespace isolation working"
      ],
      "prevention": "Configure nodemaps to automatically enforce tenant-namespace mapping. Use VLAN isolation.",
      "severity": "CRITICAL",
      "frequency": "Common in subdirectory mount configurations",
      "related_errors": ["ERR011", "ERR013", "ERR019"],
      "test_scenarios": ["Namespace isolation tests", "Nodemap configuration"],
      "tags": ["security", "namespace", "lustre", "nodemap", "403", "multi-tenancy"]
    },
    {
      "error_id": "ERR013",
      "error_type": "S3BucketAccessDenied",
      "error_category": "SECURITY",
      "subcategory": "S3 Multi-Tenancy",
      "error_message": "AccessDenied: Access Denied (Service: Amazon S3; Status Code: 403)",
      "component": "S3 Protocol Gateway",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "633-663",
      "root_cause": "S3 client attempted to access a bucket belonging to different tenant. S3 multi-tenancy isolation correctly denies cross-tenant bucket access.",
      "code_before": "// Using Tenant 1 S3 client to access Tenant 2 bucket\nconst s3Client1 = new AWS.S3({\n    accessKeyId: tenant1AccessKey,\n    secretAccessKey: tenant1SecretKey\n});\nconst objects = await s3Client1.listObjectsV2({\n    Bucket: 'tenant2-data-bucket'  // Wrong tenant!\n}).promise();",
      "code_after": "// Verify bucket ownership before access\nconst bucketInfo = await getBucketInfo('tenant2-data-bucket');\nif (bucketInfo.ownerTenant !== currentTenant.id) {\n    throw new S3AccessDeniedException(\n        `Bucket ${bucketName} belongs to tenant ${bucketInfo.ownerTenant}`\n    );\n}\n// Use correct tenant's S3 credentials\nconst s3Client = getS3ClientForTenant(currentTenant);\nconst objects = await s3Client.listObjectsV2({\n    Bucket: currentTenant.bucket\n}).promise();",
      "solution_steps": [
        "Verify S3 access key matches bucket owner",
        "Check bucket policy in DDN EMF console",
        "Review S3 multi-tenancy configuration",
        "Ensure bucket naming includes tenant identifier",
        "Test with: aws s3 ls s3://bucket-name --endpoint-url",
        "Verify IAM role/policy if using role-based access",
        "This is EXPECTED security behavior"
      ],
      "prevention": "Use tenant-scoped S3 credentials. Prefix bucket names with tenant ID.",
      "severity": "HIGH",
      "frequency": "Common in S3 multi-tenancy setups",
      "related_errors": ["ERR005", "ERR011", "ERR014"],
      "test_scenarios": ["S3 cross-tenant access tests", "Bucket isolation"],
      "tags": ["s3", "access-denied", "multi-tenancy", "security", "403"]
    },
    {
      "error_id": "ERR014",
      "error_type": "S3QuotaExceeded",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "S3 Storage Quota",
      "error_message": "HTTP 507 Insufficient Storage: S3 tenant quota exceeded",
      "component": "S3 Quota Management",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "665-698",
      "root_cause": "Tenant has consumed all allocated S3 storage quota. Hard limit prevents further uploads. Similar to filesystem quota but for S3 protocol.",
      "code_before": "// No quota check before S3 upload\nawait s3Client.putObject({\n    Bucket: 'tenant-bucket',\n    Key: 'large-file.bin',\n    Body: largeBuffer  // No size validation\n}).promise();",
      "code_after": "// Check S3 quota before upload\nconst quota = await getS3QuotaInfo(tenantId);\nconst objectSize = largeBuffer.length;\n\nif (quota.used + objectSize > quota.hardLimit) {\n    throw new S3QuotaExceededException(\n        `Upload would exceed S3 quota. ` +\n        `Used: ${quota.usedGB}GB, Available: ${quota.availableGB}GB, ` +\n        `Upload size: ${(objectSize / 1024 / 1024 / 1024).toFixed(2)}GB`\n    );\n}\n\nawait s3Client.putObject({\n    Bucket: 'tenant-bucket',\n    Key: 'large-file.bin',\n    Body: largeBuffer\n}).promise();",
      "solution_steps": [
        "Check S3 bucket usage: aws s3api list-objects --bucket --query",
        "Review tenant quota in DDN EMF console",
        "Delete unnecessary S3 objects or old versions",
        "Request quota increase from administrator",
        "Implement S3 lifecycle policies for automatic cleanup",
        "Enable S3 versioning with expiration rules",
        "Consider migrating cold data to cheaper storage tier"
      ],
      "prevention": "Monitor S3 usage proactively. Set alerts at 80% and 90% thresholds.",
      "severity": "MEDIUM",
      "frequency": "Common in multi-tenant S3 environments",
      "related_errors": ["ERR006", "ERR013"],
      "test_scenarios": ["S3 quota enforcement", "S3 capacity management"],
      "tags": ["s3", "quota", "507", "storage", "capacity"]
    },
    {
      "error_id": "ERR015",
      "error_type": "S3BucketPolicyError",
      "error_category": "CONFIGURATION",
      "subcategory": "S3 Bucket Policy",
      "error_message": "InvalidBucketPolicy: Invalid S3 bucket policy JSON",
      "component": "S3 Bucket Policy Management",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "700-745",
      "root_cause": "S3 bucket policy JSON is malformed or contains invalid IAM syntax. Common issues: incorrect ARN format, missing required fields, invalid principal.",
      "code_before": "const bucketPolicy = {\n    Version: '2012-10-17',\n    Statement: [{\n        Effect: 'Allow',\n        Principal: 'tenant1',  // WRONG: Not valid ARN format\n        Action: 's3:*',\n        Resource: 'tenant1-bucket'  // WRONG: Missing ARN prefix\n    }]\n};\nawait s3Client.putBucketPolicy({\n    Bucket: bucketName,\n    Policy: JSON.stringify(bucketPolicy)\n}).promise();",
      "code_after": "const bucketPolicy = {\n    Version: '2012-10-17',\n    Statement: [{\n        Sid: 'Tenant1Access',\n        Effect: 'Allow',\n        Principal: {\n            AWS: `arn:aws:iam::tenant1:root`  // CORRECT ARN format\n        },\n        Action: [\n            's3:GetObject',\n            's3:PutObject',\n            's3:ListBucket'\n        ],\n        Resource: [\n            `arn:aws:s3:::${bucketName}`,          // Bucket ARN\n            `arn:aws:s3:::${bucketName}/*`         // Objects ARN\n        ]\n    }]\n};\n\n// Validate policy before applying\ntry {\n    JSON.parse(JSON.stringify(bucketPolicy));\n    await s3Client.putBucketPolicy({\n        Bucket: bucketName,\n        Policy: JSON.stringify(bucketPolicy)\n    }).promise();\n} catch (e) {\n    logger.error('Invalid bucket policy:', e);\n    throw new InvalidBucketPolicyError(e.message);\n}",
      "solution_steps": [
        "Validate JSON syntax with JSON linter",
        "Use correct ARN format: arn:aws:iam::account:root",
        "Include both bucket and object resource ARNs",
        "Specify explicit actions instead of wildcard (*)",
        "Add Statement ID (Sid) for clarity",
        "Test policy with AWS Policy Simulator",
        "Review S3 bucket policy examples in DDN docs"
      ],
      "prevention": "Use bucket policy templates. Validate policies before applying.",
      "severity": "MEDIUM",
      "frequency": "Common during initial S3 setup",
      "related_errors": ["ERR004", "ERR013"],
      "test_scenarios": ["S3 bucket policy configuration", "IAM policy validation"],
      "tags": ["s3", "bucket-policy", "iam", "configuration", "validation"]
    },
    {
      "error_id": "ERR016",
      "error_type": "RootPrivilegeEscalation",
      "error_category": "SECURITY",
      "subcategory": "Root Squashing",
      "error_message": "Security violation: Root access not squashed",
      "component": "Lustre Nodemap Root Squashing",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "369-404",
      "root_cause": "Root user (UID 0) was NOT squashed to regular user as expected. This is a security vulnerability that allows privilege escalation in multi-tenant environments.",
      "code_before": "// Nodemap without root squashing (DANGEROUS)\nconst nodemap = {\n    nodemap_name: 'tenant_nodemap',\n    client_nids: ['10.100.0.0/24@tcp'],\n    fileset: '/lustre/tenant1',\n    // squash_root: false  // DEFAULT: No root squashing!\n};",
      "code_after": "// Nodemap WITH root squashing (SECURE)\nconst nodemap = {\n    nodemap_name: 'tenant_nodemap',\n    client_nids: ['10.100.0.0/24@tcp'],\n    fileset: '/lustre/tenant1',\n    domain: 'tenant1.ddn.local',\n    squash_root: true,       // REQUIRED: Enable root squashing\n    squash_uid: 1001,        // Map root to this UID\n    squash_gid: 1001,        // Map root to this GID\n    deny_unknown: true       // Deny unmapped users\n};\n\n// Verify squashing is active\nconst effectiveUid = await getEffectiveUid();\nif (effectiveUid === 0) {\n    throw new SecurityViolationError(\n        'Root squashing not active! Security risk!'\n    );\n}",
      "solution_steps": [
        "IMMEDIATELY enable root squashing in nodemap",
        "Set squash_uid and squash_gid to non-zero values",
        "Verify with: lctl nodemap_info <nodemap_name>",
        "Test root access is properly squashed",
        "Review all nodemaps for security compliance",
        "Audit recent root access attempts",
        "Update security documentation",
        "Notify security team of vulnerability"
      ],
      "prevention": "ALWAYS enable root squashing in multi-tenant environments. Make it default in configs.",
      "severity": "CRITICAL",
      "frequency": "Rare but extremely serious",
      "related_errors": ["ERR012", "ERR019"],
      "test_scenarios": ["Root squashing tests", "Security compliance"],
      "tags": ["security", "root-squashing", "nodemap", "privilege-escalation", "critical"]
    },
    {
      "error_id": "ERR017",
      "error_type": "VLANIsolationFailure",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "Network Isolation",
      "error_message": "VLAN isolation not enforced: Cross-VLAN traffic detected",
      "component": "Network VLAN Configuration",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "184-220",
      "root_cause": "Network traffic is crossing VLAN boundaries, indicating VLAN isolation is not properly configured. Tenants on different VLANs should NOT be able to communicate.",
      "code_before": "// No VLAN validation\nconst networkConfig = {\n    tenant1_network: '10.100.0.0/24',\n    tenant2_network: '10.200.0.0/24'\n    // Missing VLAN IDs!\n};",
      "code_after": "// Proper VLAN configuration\nconst networkConfig = {\n    tenant1: {\n        network: '10.100.0.0/24',\n        vlan_id: 100,              // Dedicated VLAN\n        gateway: '10.100.0.1',\n        isolated: true\n    },\n    tenant2: {\n        network: '10.200.0.0/24',\n        vlan_id: 200,              // Different VLAN\n        gateway: '10.200.0.1',\n        isolated: true\n    }\n};\n\n// Verify VLAN isolation\nconst canRoute = await testCrossVLANRouting(100, 200);\nif (canRoute) {\n    throw new SecurityViolationError(\n        'VLAN isolation breach: VLAN 100 can reach VLAN 200!'\n    );\n}",
      "solution_steps": [
        "Verify switch VLAN configuration",
        "Check VLAN tagging is enabled on trunk ports",
        "Ensure inter-VLAN routing is disabled",
        "Verify firewall rules between VLANs",
        "Test with: ping from Tenant 1 to Tenant 2",
        "Review network topology diagram",
        "Update VLAN database on switches",
        "Verify access ports are assigned to correct VLANs"
      ],
      "prevention": "Use 802.1Q VLAN tagging. Disable inter-VLAN routing. Regular network audits.",
      "severity": "HIGH",
      "frequency": "Common in initial network setup",
      "related_errors": ["ERR011", "ERR012"],
      "test_scenarios": ["VLAN isolation tests", "Network security"],
      "tags": ["network", "vlan", "isolation", "security", "multi-tenancy"]
    },
    {
      "error_id": "ERR018",
      "error_type": "KerberosAuthenticationFailure",
      "error_category": "CONFIGURATION",
      "subcategory": "Kerberos Authentication",
      "error_message": "HTTP 401 Unauthorized: Kerberos authentication failed",
      "component": "Kerberos Authentication",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "755-786",
      "root_cause": "Kerberos authentication failed. Common causes: KDC not reachable, incorrect principal name, expired ticket, wrong realm, clock skew, or missing keytab.",
      "code_before": "// Basic auth without Kerberos validation\nconst authResponse = await authenticate({\n    username: 'tenant1_user',\n    password: 'password123'\n    // No Kerberos configuration\n});",
      "code_after": "// Proper Kerberos authentication\nconst kerberosConfig = {\n    principal: `${username}@${KERBEROS_REALM}`,  // e.g., tenant1_user@DDN.LOCAL\n    kdc_server: process.env.KERBEROS_KDC,       // e.g., kdc.ddn.local\n    service: 'lustre',\n    realm: 'DDN.LOCAL',\n    use_keytab: true,\n    keytab_path: '/etc/krb5.keytab'\n};\n\n// Verify clock sync (critical for Kerberos)\nconst clockSkew = await checkClockSkew(kerberosConfig.kdc_server);\nif (Math.abs(clockSkew) > 300) {  // 5 minutes\n    throw new KerberosClockSkewError(\n        `Clock skew too large: ${clockSkew}s. Sync with NTP server.`\n    );\n}\n\ntry {\n    const authResponse = await authenticateKerberos(kerberosConfig);\n    // Verify ticket validity\n    if (authResponse.ticketExpiry < Date.now()) {\n        throw new KerberosTicketExpiredError('Kerberos ticket expired');\n    }\n} catch (e) {\n    logger.error('Kerberos auth failed:', e);\n    throw new KerberosAuthenticationError(\n        `Authentication failed for ${kerberosConfig.principal}: ${e.message}`\n    );\n}",
      "solution_steps": [
        "Verify KDC is reachable: telnet kdc.ddn.local 88",
        "Check principal exists: kadmin -q 'get_principal user@REALM'",
        "Verify Kerberos realm is correct (case-sensitive!)",
        "Check clock synchronization: ntpdate -q kdc.server",
        "Clock skew must be < 5 minutes",
        "Verify keytab permissions: chmod 600 /etc/krb5.keytab",
        "Test with kinit: kinit user@REALM",
        "Check Kerberos config: /etc/krb5.conf"
      ],
      "prevention": "Use NTP for clock synchronization. Monitor keytab expiration. Test authentication regularly.",
      "severity": "HIGH",
      "frequency": "Common with Kerberos setup",
      "related_errors": ["ERR004", "ERR019"],
      "test_scenarios": ["Kerberos authentication", "Security compliance"],
      "tags": ["kerberos", "authentication", "401", "security", "clock-skew"]
    },
    {
      "error_id": "ERR019",
      "error_type": "NIDSpoofingAttempt",
      "error_category": "SECURITY",
      "subcategory": "Network Identity",
      "error_message": "Security violation: NID spoofing detected",
      "component": "Lustre NID Validation",
      "file_path": "tests/ddn-advanced-scenarios.js",
      "line_range": "788-824",
      "root_cause": "Client is using a valid NID from one tenant's network but presenting credentials from a different tenant. This is a security attack attempting to bypass network-based access control.",
      "code_before": "// No NID-to-tenant validation\nfunction authenticateClient(nid, credentials) {\n    // Only checks credentials, ignores NID\n    return validateCredentials(credentials);\n}",
      "code_after": "// Validate NID matches tenant credentials\nfunction authenticateClient(nid, credentials) {\n    // Extract tenant from credentials\n    const tenantFromCreds = extractTenant(credentials.principal);\n    \n    // Get expected NID range for this tenant\n    const expectedNIDRange = getTenantNIDRange(tenantFromCreds);\n    \n    // Verify NID is within tenant's range\n    if (!isNIDInRange(nid, expectedNIDRange)) {\n        logSecurityEvent({\n            type: 'NID_SPOOFING_ATTEMPT',\n            nid: nid,\n            tenant: tenantFromCreds,\n            expected_range: expectedNIDRange,\n            severity: 'CRITICAL'\n        });\n        \n        throw new NIDSpoofingError(\n            `NID ${nid} is not authorized for tenant ${tenantFromCreds}. ` +\n            `Expected NID range: ${expectedNIDRange}`\n        );\n    }\n    \n    // Also verify with Kerberos for double security\n    return validateCredentials(credentials);\n}",
      "solution_steps": [
        "IMMEDIATELY block the client IP address",
        "Review security logs for other spoofing attempts",
        "Verify nodemap configuration restricts NID ranges",
        "Enable Kerberos authentication (prevents NID spoofing)",
        "Check firewall rules allow only authorized networks",
        "Audit all client access from suspicious networks",
        "Notify security team immediately",
        "Consider legal action if attack persists"
      ],
      "prevention": "Always use Kerberos with nodemap. NID alone is NOT secure. Monitor for anomalies.",
      "severity": "CRITICAL",
      "frequency": "Rare (indicates attack)",
      "related_errors": ["ERR012", "ERR016", "ERR018"],
      "test_scenarios": ["NID spoofing prevention", "Security compliance"],
      "tags": ["security", "nid-spoofing", "attack", "critical", "nodemap"]
    },
    {
      "error_id": "ERR020",
      "error_type": "PerformanceBenchmarkFailure",
      "error_category": "PERFORMANCE",
      "subcategory": "Throughput",
      "error_message": "Performance assertion failed: Expected 4x speedup, got 2.1x",
      "component": "AI400X Data Loading",
      "file_path": "tests/ddn-test-scenarios.js",
      "line_range": "827-863",
      "root_cause": "AI400X data loading performance did not meet the claimed 4x speedup. Actual speedup was only 2.1x. Could indicate: incorrect configuration, hardware bottleneck, or unrealistic baseline.",
      "code_before": "// Simple performance test without baselines\nconst throughput = await measureDataLoading();\nexpect(throughput).to.be.greaterThan(baselineThroughput * 4);  // Hard assertion",
      "code_after": "// Comprehensive performance testing\nconst config = {\n    dataset_size_gb: 100,\n    batch_size: 256,\n    num_workers: 32,\n    warmup_runs: 3,      // Warm up cache\n    test_runs: 10        // Multiple runs for average\n};\n\n// Measure baseline (standard storage)\nconst baselineResults = await measureDataLoading(config, 'standard');\nconst baselineThroughput = baselineResults.avg_throughput;\n\n// Measure AI400X performance\nconst ai400xResults = await measureDataLoading(config, 'ai400x');\nconst ai400xThroughput = ai400xResults.avg_throughput;\n\nconst speedup = ai400xThroughput / baselineThroughput;\n\nlogger.info(`Baseline: ${baselineThroughput} samples/sec`);\nlogger.info(`AI400X: ${ai400xThroughput} samples/sec`);\nlogger.info(`Speedup: ${speedup.toFixed(2)}x`);\n\n// Flexible assertion with detailed failure info\nif (speedup < 3.5) {  // Allow some variance\n    throw new PerformanceRegressionError(\n        `AI400X speedup ${speedup.toFixed(2)}x is below target 4x. ` +\n        `Check: 1) GPU Direct Storage enabled, 2) Network bandwidth, ` +\n        `3) Storage load, 4) Batch size optimization`\n    );\n}",
      "solution_steps": [
        "Verify GPU Direct Storage is enabled",
        "Check network bandwidth: iperf3 test",
        "Monitor storage load during test",
        "Increase batch size for better parallelism",
        "Verify AI400X-specific optimizations are active",
        "Check for competing workloads",
        "Review dataset format (TFRecord vs raw)",
        "Optimize data pipeline configuration"
      ],
      "prevention": "Use realistic baselines. Test with warmed-up cache. Allow performance variance.",
      "severity": "MEDIUM",
      "frequency": "Common in performance testing",
      "related_errors": ["ERR021", "ERR010"],
      "test_scenarios": ["AI400X performance benchmarks", "Data loading tests"],
      "tags": ["performance", "ai400x", "throughput", "benchmark", "optimization"]
    },
    {
      "error_id": "ERR021",
      "error_type": "LatencyThresholdExceeded",
      "error_category": "PERFORMANCE",
      "subcategory": "Latency",
      "error_message": "Latency threshold exceeded: p99=650μs (expected <500μs)",
      "component": "AI400X Storage",
      "file_path": "tests/ddn-test-scenarios.js",
      "line_range": "865-901",
      "root_cause": "99th percentile (p99) latency exceeds the <100μs target. While average latency may be good, tail latencies are too high. This affects worst-case training performance.",
      "code_before": "// Only checking average latency\nconst avgLatency = await measureAverageLatency();\nexpect(avgLatency).to.be.lessThan(100);  // Not enough!",
      "code_after": "// Comprehensive latency testing with percentiles\nconst latencyResults = await measureLatencyDistribution({\n    operation: 'random_read',\n    block_size: '4K',\n    queue_depth: 32,\n    duration_sec: 60,\n    percentiles: [50, 90, 95, 99, 99.9]\n});\n\nlogger.info('Latency Distribution:');\nlogger.info(`  p50 (median): ${latencyResults.p50}μs`);\nlogger.info(`  p90: ${latencyResults.p90}μs`);\nlogger.info(`  p95: ${latencyResults.p95}μs`);\nlogger.info(`  p99: ${latencyResults.p99}μs`);\nlogger.info(`  p99.9: ${latencyResults.p999}μs`);\n\n// Check ALL percentiles, not just average\nconst checks = [\n    { metric: 'avg_latency', value: latencyResults.avg, threshold: 100 },\n    { metric: 'p99_latency', value: latencyResults.p99, threshold: 500 },\n    { metric: 'p99.9_latency', value: latencyResults.p999, threshold: 2000 }\n];\n\nconst failures = checks.filter(c => c.value > c.threshold);\nif (failures.length > 0) {\n    throw new LatencyThresholdError(\n        `Latency targets not met:\\n` +\n        failures.map(f => `  ${f.metric}: ${f.value}μs > ${f.threshold}μs`).join('\\n') +\n        `\\n\\nCheck: 1) Disk I/O stats, 2) Queue depth tuning, 3) Cache hit rate`\n    );\n}",
      "solution_steps": [
        "Check disk I/O statistics: iostat -x 1",
        "Review cache hit rate (low cache hits = high latency)",
        "Tune queue depth settings",
        "Verify NVMe drives are healthy: smartctl",
        "Check for storage contention from other workloads",
        "Review read-ahead and prefetch settings",
        "Consider SSD wear leveling status",
        "Optimize block size for workload"
      ],
      "prevention": "Monitor tail latencies, not just averages. Use percentile-based SLAs.",
      "severity": "HIGH",
      "frequency": "Common under load",
      "related_errors": ["ERR010", "ERR020"],
      "test_scenarios": ["Latency benchmarks", "Performance SLAs"],
      "tags": ["performance", "latency", "p99", "tail-latency", "optimization"]
    },
    {
      "error_id": "ERR022",
      "error_type": "LustreScalabilityFailure",
      "error_category": "PERFORMANCE",
      "subcategory": "Parallel I/O",
      "error_message": "Lustre scalability test failed: Expected 10x speedup at 32 processes, got 6.5x",
      "component": "EXAScaler Lustre",
      "file_path": "tests/ddn-test-scenarios.js",
      "line_range": "903-943",
      "root_cause": "Lustre parallel I/O is not scaling linearly. At 32 parallel processes, expected 10x+ speedup but only achieved 6.5x. Indicates: insufficient striping, OSS bottleneck, or lock contention.",
      "code_before": "// Single striping configuration for all tests\nconst file = createFile({\n    stripe_count: 1,  // No parallelism!\n    stripe_size: '1M'\n});",
      "code_after": "// Optimize striping for parallel workloads\nfunction getOptimalStriping(parallelism, fileSize) {\n    // Rule: stripe_count should match or exceed parallelism\n    const baseStripeCount = Math.min(parallelism, availableOSSCount);\n    \n    // For very large files, increase stripe count\n    const fileSizeGB = fileSize / (1024 ** 3);\n    const stripeCount = fileSizeGB > 100 ? \n        Math.min(baseStripeCount * 2, availableOSSCount) : \n        baseStripeCount;\n    \n    // Larger stripe size for large files\n    const stripeSize = fileSizeGB > 100 ? '16M' : '4M';\n    \n    return { stripe_count: stripeCount, stripe_size: stripeSize };\n}\n\nconst striping = getOptimalStriping(32, 100 * 1024 ** 3);  // 32 processes, 100GB file\nconst file = createFile(striping);\n\n// Verify striping was applied\nconst actualStriping = await getFileStriping(file.path);\nlogger.info(`Stripe count: ${actualStriping.stripe_count}`);\nlogger.info(`Stripe size: ${actualStriping.stripe_size}`);\n\n// Run parallel I/O test\nconst results = await runParallelIOTest({\n    file_path: file.path,\n    parallel_processes: [1, 2, 4, 8, 16, 32],\n    operation: 'write'\n});\n\n// Verify scaling efficiency\nconst speedup32 = results[32].throughput / results[1].throughput;\nif (speedup32 < parallelism * 0.3) {  // At least 30% efficiency\n    throw new ScalabilityError(\n        `Poor parallel scaling: ${speedup32.toFixed(1)}x at 32 processes. ` +\n        `Check: 1) Stripe count matches parallelism, 2) OSS load balance, ` +\n        `3) Lock contention with lctl get_param ldlm.*`\n    );\n}",
      "solution_steps": [
        "Increase stripe count to match parallelism",
        "Check OSS load distribution: lfs df -h",
        "Verify no single OSS is bottleneck",
        "Review Lustre lock statistics: lctl get_param ldlm.*",
        "Use progressive file layout for very large files",
        "Check network bandwidth between clients and OSS",
        "Tune Lustre client and OSS parameters",
        "Consider adding more OSS servers"
      ],
      "prevention": "Match stripe count to workload parallelism. Monitor OSS load distribution.",
      "severity": "MEDIUM",
      "frequency": "Common with high parallelism",
      "related_errors": ["ERR008", "ERR009", "ERR020"],
      "test_scenarios": ["Lustre parallel I/O", "Scalability tests"],
      "tags": ["lustre", "scalability", "parallel-io", "performance", "striping"]
    },
    {
      "error_id": "ERR023",
      "error_type": "ClusterHealthDegraded",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "Cluster Health",
      "error_message": "Cluster health check failed: 2 of 8 OSS servers offline",
      "component": "EXAScaler Cluster",
      "file_path": "tests/ddn-test-scenarios.js",
      "line_range": "98-123",
      "root_cause": "Lustre cluster health is degraded. Some OSS (Object Storage Servers) or MDS (Metadata Servers) are offline or unreachable. This reduces cluster capacity and performance.",
      "code_before": "// No cluster health validation before operations\nconst response = await lustreClient.writeData(data);",
      "code_after": "// Check cluster health before critical operations\nconst clusterStatus = await getClusterStatus();\n\nlogger.info('Cluster Status:');\nlogger.info(`  MDS Servers: ${clusterStatus.mds_active}/${clusterStatus.mds_total}`);\nlogger.info(`  OSS Servers: ${clusterStatus.oss_active}/${clusterStatus.oss_total}`);\nlogger.info(`  Overall Health: ${clusterStatus.health}`);\n\n// Check if cluster is healthy enough\nconst ossHealthPercent = (clusterStatus.oss_active / clusterStatus.oss_total) * 100;\nconst mdsHealthPercent = (clusterStatus.mds_active / clusterStatus.mds_total) * 100;\n\nif (mdsHealthPercent < 100) {\n    throw new ClusterDegradedError(\n        `MDS degraded: ${clusterStatus.mds_active}/${clusterStatus.mds_total} servers active. ` +\n        `Metadata operations may fail. Contact storage admin immediately.`\n    );\n}\n\nif (ossHealthPercent < 75) {\n    throw new ClusterDegradedError(\n        `OSS degraded: ${clusterStatus.oss_active}/${clusterStatus.oss_total} servers active ` +\n        `(${ossHealthPercent.toFixed(0)}%). Storage capacity and performance reduced. ` +\n        `Offline servers: ${clusterStatus.oss_offline.join(', ')}`\n    );\n}\n\nif (ossHealthPercent < 90) {\n    logger.warn(`OSS health at ${ossHealthPercent.toFixed(0)}%. Performance may be affected.`);\n}\n\nconst response = await lustreClient.writeData(data);",
      "solution_steps": [
        "Check OSS status: lctl dl | grep OST",
        "Check MDS status: lctl dl | grep MDT",
        "Identify offline servers from cluster status",
        "Check network connectivity to offline servers",
        "Review server logs: /var/log/lustre/",
        "Restart failed services: systemctl restart lustre-oss",
        "Verify hardware status (disks, network, power)",
        "Contact DDN support if hardware failure"
      ],
      "prevention": "Monitor cluster health continuously. Set up alerts for server failures.",
      "severity": "HIGH",
      "frequency": "Occasional hardware/network issues",
      "related_errors": ["ERR009", "ERR010"],
      "test_scenarios": ["Cluster health checks", "High availability"],
      "tags": ["lustre", "cluster", "health", "oss", "mds", "infrastructure"]
    },
    {
      "error_id": "ERR024",
      "error_type": "CheckpointStorageFailure",
      "error_category": "INFRASTRUCTURE",
      "subcategory": "AI Checkpoint Management",
      "error_message": "Failed to store AI model checkpoint: Storage unavailable",
      "component": "AI400X Checkpoint Storage",
      "file_path": "tests/ddn-test-scenarios.js",
      "line_range": "258-303",
      "root_cause": "AI model checkpoint could not be stored. Common causes: storage full, permissions issue, storage system offline, network failure, or checkpoint too large.",
      "code_before": "// No error handling for checkpoint storage\nawait storeCheckpoint({\n    model_name: modelName,\n    checkpoint_data: checkpointBuffer\n});  // Fails silently if storage unavailable",
      "code_after": "// Robust checkpoint storage with fallback\nasync function storeCheckpointWithRetry(checkpointConfig) {\n    const { model_name, checkpoint_data, epoch } = checkpointConfig;\n    \n    // Check storage health first\n    const storageStatus = await checkStorageHealth();\n    if (!storageStatus.available) {\n        throw new StorageUnavailableError(\n            `Primary storage unavailable: ${storageStatus.reason}`\n        );\n    }\n    \n    // Check available space\n    const checkpointSize = checkpoint_data.length;\n    if (storageStatus.available_bytes < checkpointSize * 1.1) {  // 10% buffer\n        throw new InsufficientStorageError(\n            `Insufficient space: Need ${(checkpointSize / 1024 ** 3).toFixed(2)}GB, ` +\n            `Available ${(storageStatus.available_bytes / 1024 ** 3).toFixed(2)}GB`\n        );\n    }\n    \n    // Try primary storage with retries\n    for (let attempt = 1; attempt <= 3; attempt++) {\n        try {\n            const checkpointId = await storeCheckpoint({\n                model_name,\n                checkpoint_data,\n                storage_tier: 'fast',\n                metadata: {\n                    epoch,\n                    timestamp: new Date().toISOString(),\n                    size_bytes: checkpointSize\n                }\n            });\n            \n            // Verify checksum\n            await verifyCheckpointIntegrity(checkpointId);\n            \n            logger.info(`Checkpoint stored: ${checkpointId} (epoch ${epoch})`);\n            return checkpointId;\n            \n        } catch (e) {\n            logger.warn(`Checkpoint storage attempt ${attempt}/3 failed: ${e.message}`);\n            if (attempt < 3) {\n                await sleep(5000 * attempt);  // Exponential backoff\n            } else {\n                // Try fallback storage\n                logger.warn('Trying fallback storage...');\n                return await storeCheckpointToFallback(checkpointConfig);\n            }\n        }\n    }\n}",
      "solution_steps": [
        "Check storage system health and availability",
        "Verify available storage space: df -h",
        "Check network connectivity to storage",
        "Review checkpoint storage permissions",
        "Verify checkpoint size is within limits",
        "Clean up old checkpoints if space is low",
        "Use checkpoint compression if available",
        "Configure automatic failover to backup storage"
      ],
      "prevention": "Monitor storage capacity. Implement checkpoint rotation. Use fallback storage.",
      "severity": "HIGH",
      "frequency": "Occasional during training",
      "related_errors": ["ERR006", "ERR007", "ERR014"],
      "test_scenarios": ["AI checkpoint storage", "Storage failover"],
      "tags": ["ai400x", "checkpoint", "storage", "training", "failover"]
    },
    {
      "error_id": "ERR025",
      "error_type": "InfiniaOptimizationFailure",
      "error_category": "CONFIGURATION",
      "subcategory": "Workload Optimization",
      "error_message": "Infinia workload optimization failed: Unable to generate optimization profile",
      "component": "DDN Infinia",
      "file_path": "tests/ddn-test-scenarios.js",
      "line_range": "404-436",
      "root_cause": "Infinia could not generate workload optimization profile. Common causes: insufficient workload information, incompatible model configuration, or service unavailable.",
      "code_before": "// Minimal workload info (insufficient)\nconst profile = await optimizeWorkload({\n    workload_type: 'llm_training'\n    // Missing: model size, GPU count, throughput target\n});",
      "code_after": "// Comprehensive workload configuration\nconst workloadConfig = {\n    workload_type: 'llm_training',\n    model_size: '70B',              // Model parameters\n    model_architecture: 'transformer',\n    gpus: 64,\n    gpu_type: 'A100-80GB',\n    nodes: 8,\n    training_framework: 'pytorch',\n    distributed_strategy: 'FSDP',   // Fully Sharded Data Parallel\n    expected_tokens_per_sec: 10000,\n    dataset_size_tb: 5,\n    checkpoint_frequency_min: 60,\n    precision: 'bf16',\n    batch_size_per_gpu: 4,\n    sequence_length: 4096\n};\n\n// Validate configuration before optimization\nconst validation = await validateWorkloadConfig(workloadConfig);\nif (!validation.valid) {\n    throw new InvalidWorkloadConfigError(\n        `Workload configuration invalid: ${validation.errors.join(', ')}`\n    );\n}\n\ntry {\n    const optimizationProfile = await optimizeWorkload(workloadConfig);\n    \n    logger.info('Optimization Profile Generated:');\n    logger.info(`  Data Pipeline: ${optimizationProfile.data_pipeline_config.prefetch_factor}x prefetch`);\n    logger.info(`  Storage Tiering: ${optimizationProfile.storage_tiering.fast_tier_percent}% on fast tier`);\n    logger.info(`  Checkpoint Strategy: ${optimizationProfile.checkpoint_strategy.frequency}`);\n    logger.info(`  Expected Training Time: ${optimizationProfile.estimated_training_time_hours}h`);\n    \n    return optimizationProfile;\n    \n} catch (e) {\n    throw new InfiniaOptimizationError(\n        `Failed to generate optimization profile: ${e.message}. ` +\n        `Verify: 1) Infinia service running, 2) Workload config complete, ` +\n        `3) Sufficient cluster resources`\n    );\n}",
      "solution_steps": [
        "Verify Infinia service is running and healthy",
        "Provide complete workload configuration",
        "Check model size is supported (up to 70B+)",
        "Verify GPU count matches cluster capacity",
        "Review training framework compatibility",
        "Check Infinia version supports workload type",
        "Review Infinia logs for specific errors",
        "Consult Infinia documentation for workload examples"
      ],
      "prevention": "Use workload configuration templates. Validate configs before optimization.",
      "severity": "MEDIUM",
      "frequency": "Common during initial setup",
      "related_errors": ["ERR020", "ERR024"],
      "test_scenarios": ["Infinia workload optimization", "LLM training"],
      "tags": ["infinia", "optimization", "llm", "ai-workload", "configuration"]
    }
  ]
}
