# ACTUAL DDN AI Workflow - Corrected Version

**Date**: October 21, 2025
**Purpose**: Document the ACTUAL workflow based on existing implementation
**Status**: Corrected based on your clarifications

---

## ğŸ¯ Your Clarifications

### âœ… What You Told Me:

1. **Data Storage**: Jenkins build data is stored via **API scripts** (NOT via n8n webhooks)
2. **Manual Trigger**: Users can **bypass 3-day aging** via Dashboard
3. **Workflow Trigger**: Both 3-day aging AND manual trigger can start the analysis

---

## ğŸ”„ ACTUAL Complete Workflow

### **Path 1: Automatic 3-Day Aging (Default)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: Build Fails & Data Storage (via API Scripts)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ Time: T+0 (Build fails at 10:00 AM Monday)                     â”‚
â”‚                                                                  â”‚
â”‚ Jenkins Build #12345 Fails                                     â”‚
â”‚    â†“                                                             â”‚
â”‚ Jenkins calls API Script (Python/Shell)                        â”‚
â”‚    â†“                                                             â”‚
â”‚ API Script stores data in:                                      â”‚
â”‚    â€¢ PostgreSQL:                                                â”‚
â”‚      - build_metadata table                                     â”‚
â”‚      - aging_status = 'PENDING'                                 â”‚
â”‚      - aging_days = 0                                           â”‚
â”‚      - created_at = '2025-01-20 10:00:00'                      â”‚
â”‚                                                                  â”‚
â”‚    â€¢ MongoDB:                                                   â”‚
â”‚      - console_logs collection (full console output)            â”‚
â”‚      - error_details collection (stack traces)                  â”‚
â”‚      - build_context collection                                 â”‚
â”‚        â€¢ GitHub code files                                      â”‚
â”‚        â€¢ Test scripts                                           â”‚
â”‚        â€¢ Knowledge documents (README, docs)                     â”‚
â”‚                                                                  â”‚
â”‚ âœ… DATA STORED - No n8n involvement yet!                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: Aging Period (Day 0 to Day 3)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ Day 0 (Monday):                                                 â”‚
â”‚    aging_status = 'PENDING', aging_days = 0                     â”‚
â”‚                                                                  â”‚
â”‚ Day 1 (Tuesday):                                                â”‚
â”‚    Daily cron job updates: aging_days = 1                       â”‚
â”‚                                                                  â”‚
â”‚ Day 2 (Wednesday):                                              â”‚
â”‚    Daily cron job updates: aging_days = 2                       â”‚
â”‚                                                                  â”‚
â”‚ Day 3 (Thursday): âš¡ TRIGGER POINT                              â”‚
â”‚    Daily cron job updates:                                      â”‚
â”‚      aging_status = 'READY_FOR_ANALYSIS'                        â”‚
â”‚      aging_days = 3                                             â”‚
â”‚    â†“                                                             â”‚
â”‚    Cron job calls n8n webhook:                                  â”‚
â”‚      POST http://localhost:5678/webhook/ddn-test-failure       â”‚
â”‚      {                                                           â”‚
â”‚        "build_id": "BUILD_12345",                               â”‚
â”‚        "trigger_type": "automatic_aging",                       â”‚
â”‚        "aging_days": 3                                          â”‚
â”‚      }                                                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: AI Analysis (n8n + LangGraph + RAG/MCP)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ n8n Workflow Starts                                             â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 1: Fetch build data from MongoDB                          â”‚
â”‚    â€¢ Get full console logs                                      â”‚
â”‚    â€¢ Get error details with stack traces                        â”‚
â”‚    â€¢ Get GitHub code files                                      â”‚
â”‚    â€¢ Get test scripts                                           â”‚
â”‚    â€¢ Get knowledge documents                                    â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 2: Call LangGraph Service                                  â”‚
â”‚    POST http://localhost:5000/classify-error                    â”‚
â”‚    {                                                             â”‚
â”‚      "build_id": "BUILD_12345",                                 â”‚
â”‚      "error_log": "OutOfMemoryError: Java heap space...",      â”‚
â”‚      "full_context": {                                          â”‚
â”‚        "stack_trace": "...",                                    â”‚
â”‚        "console_output": "...",                                 â”‚
â”‚        "github_files": [...],                                   â”‚
â”‚        "test_scripts": [...]                                    â”‚
â”‚      }                                                           â”‚
â”‚    }                                                             â”‚
â”‚    â†“                                                             â”‚
â”‚ LangGraph Classification Agent:                                 â”‚
â”‚    Step 1: Classify error type                                  â”‚
â”‚      â€¢ Keyword matching                                         â”‚
â”‚      â€¢ Result: INFRA_ERROR (OutOfMemoryError)                  â”‚
â”‚      â€¢ Confidence: 0.95                                         â”‚
â”‚      â€¢ needs_code_analysis: FALSE                              â”‚
â”‚    â†“                                                             â”‚
â”‚    Step 2: Generate Embedding (OpenAI)                          â”‚
â”‚      â€¢ Input: "OutOfMemoryError: Java heap space..."           â”‚
â”‚      â€¢ Output: [0.234, -0.567, ..., 0.789] (1536 dims)        â”‚
â”‚    â†“                                                             â”‚
â”‚    Step 3: RAG Search in Pinecone                               â”‚
â”‚      â€¢ Query with embedding + filter (INFRA_ERROR)             â”‚
â”‚      â€¢ Found 5 similar past errors                              â”‚
â”‚      â€¢ Best match: 0.95 similarity, 92% success rate           â”‚
â”‚    â†“                                                             â”‚
â”‚    Decision: RAG or MCP?                                        â”‚
â”‚      âœ… INFRA_ERROR + high similarity â†’ USE RAG                â”‚
â”‚      âŒ CODE_ERROR or low similarity â†’ USE MCP                 â”‚
â”‚    â†“                                                             â”‚
â”‚ RAG PATH (for this example):                                    â”‚
â”‚    â€¢ Select best solution from Pinecone                         â”‚
â”‚    â€¢ Root cause: "JVM heap insufficient"                        â”‚
â”‚    â€¢ Fix: "Increase heap to 4GB with -Xmx4g"                   â”‚
â”‚    â€¢ Time: 5 seconds                                            â”‚
â”‚    â€¢ Cost: $0.01                                                â”‚
â”‚    â€¢ NO GitHub MCP needed! âœ“                                    â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 3: Store solution in MongoDB                               â”‚
â”‚    â€¢ analysis_solutions collection                              â”‚
â”‚    â€¢ Include similarity score, success rate                     â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 4: Update PostgreSQL                                       â”‚
â”‚    â€¢ analysis_status = 'ANALYZED'                               â”‚
â”‚    â€¢ analysis_type = 'RAG'                                      â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 5: Store in Pinecone (for future RAG)                      â”‚
â”‚    â€¢ Update success rate if solution used before                â”‚
â”‚    â€¢ OR store as new vector if new solution                     â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 6: Send Teams Notification                                 â”‚
â”‚    â€¢ Solution ready                                             â”‚
â”‚    â€¢ Include root cause, fix, confidence                        â”‚
â”‚    â†“                                                             â”‚
â”‚ Node 7: Update Dashboard via API                                â”‚
â”‚    â€¢ POST http://localhost:5005/api/failures/BUILD_12345       â”‚
â”‚    â€¢ Dashboard shows solution with clickable links              â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Path 2: Manual Trigger from Dashboard (Bypass Aging)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MANUAL TRIGGER WORKFLOW (No 3-Day Wait!)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ Scenario: User sees BUILD_12345 failed on Day 0                 â”‚
â”‚           Wants immediate analysis (can't wait 3 days)          â”‚
â”‚                                                                  â”‚
â”‚ User Action:                                                     â”‚
â”‚    Opens Dashboard â†’ Finds BUILD_12345 â†’ Clicks "Analyze Now"  â”‚
â”‚    â†“                                                             â”‚
â”‚ Dashboard Frontend (React):                                      â”‚
â”‚    POST http://localhost:5005/api/trigger/manual                â”‚
â”‚    {                                                             â”‚
â”‚      "build_id": "BUILD_12345",                                 â”‚
â”‚      "triggered_by_user": "john.doe@company.com",               â”‚
â”‚      "reason": "Critical production issue",                     â”‚
â”‚      "trigger_source": "dashboard"                              â”‚
â”‚    }                                                             â”‚
â”‚    â†“                                                             â”‚
â”‚ Dashboard API (dashboard_api.py:5005):                          â”‚
â”‚    â€¢ Receives request                                           â”‚
â”‚    â€¢ Proxies to Manual Trigger API                             â”‚
â”‚    â†“                                                             â”‚
â”‚ Manual Trigger API (manual_trigger_api.py:5004):                â”‚
â”‚    â€¢ Validates build_id exists in PostgreSQL                    â”‚
â”‚    â€¢ Logs manual trigger in manual_trigger_log table           â”‚
â”‚    â€¢ Records: user, reason, timestamp                           â”‚
â”‚    â†“                                                             â”‚
â”‚    â€¢ Calls n8n webhook:                                         â”‚
â”‚      POST http://localhost:5678/webhook/ddn-test-failure       â”‚
â”‚      {                                                           â”‚
â”‚        "build_id": "BUILD_12345",                               â”‚
â”‚        "manual_trigger": true,  â† KEY FLAG                     â”‚
â”‚        "triggered_by_user": "john.doe@company.com",             â”‚
â”‚        "trigger_reason": "Critical production issue",           â”‚
â”‚        "trigger_id": 123                                        â”‚
â”‚      }                                                           â”‚
â”‚    â†“                                                             â”‚
â”‚ n8n Workflow Starts:                                            â”‚
â”‚    â€¢ Checks manual_trigger flag                                 â”‚
â”‚    â€¢ Bypasses all aging checks                                  â”‚
â”‚    â€¢ Proceeds directly to analysis                              â”‚
â”‚    â†“                                                             â”‚
â”‚ [Same as Phase 3 above: Fetch â†’ LangGraph â†’ RAG/MCP â†’ Store â†’ Notify]
â”‚                                                                  â”‚
â”‚ Time Saved: Immediate (vs 3 days wait) âœ…                       â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Storage Architecture - CORRECTED

### **How Jenkins Data Is ACTUALLY Stored**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JENKINS BUILD FAILS                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ Jenkins Post-Build Action:                                      â”‚
â”‚    â€¢ Execute Shell Script / Python Script                       â”‚
â”‚    â€¢ OR Jenkins Webhook Plugin (but calls API, not n8n!)       â”‚
â”‚                                                                  â”‚
â”‚ Example: Jenkins Post-Build Script                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ #!/bin/bash                                            â”‚     â”‚
â”‚ â”‚                                                        â”‚     â”‚
â”‚ â”‚ if [ "$BUILD_RESULT" == "FAILURE" ]; then            â”‚     â”‚
â”‚ â”‚   # Call API script to store data                    â”‚     â”‚
â”‚ â”‚   python /path/to/store_build_failure.py \           â”‚     â”‚
â”‚ â”‚     --build-id "$BUILD_ID" \                         â”‚     â”‚
â”‚ â”‚     --build-number "$BUILD_NUMBER" \                 â”‚     â”‚
â”‚ â”‚     --job-name "$JOB_NAME" \                         â”‚     â”‚
â”‚ â”‚     --console-log "/path/to/console.log" \           â”‚     â”‚
â”‚ â”‚     --error-type "BUILD_FAILURE"                     â”‚     â”‚
â”‚ â”‚ fi                                                    â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                  â”‚
â”‚ API Script (store_build_failure.py):                           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚ â”‚ import psycopg2                                       â”‚     â”‚
â”‚ â”‚ import pymongo                                         â”‚     â”‚
â”‚ â”‚ import requests                                        â”‚     â”‚
â”‚ â”‚                                                        â”‚     â”‚
â”‚ â”‚ # 1. Store in PostgreSQL                             â”‚     â”‚
â”‚ â”‚ conn = psycopg2.connect(POSTGRES_URI)                â”‚     â”‚
â”‚ â”‚ cursor.execute("""                                    â”‚     â”‚
â”‚ â”‚   INSERT INTO build_metadata                         â”‚     â”‚
â”‚ â”‚   (build_id, build_number, job_name, status,        â”‚     â”‚
â”‚ â”‚    created_at, aging_status, aging_days)             â”‚     â”‚
â”‚ â”‚   VALUES (%s, %s, %s, 'FAILURE', NOW(),             â”‚     â”‚
â”‚ â”‚           'PENDING', 0)                               â”‚     â”‚
â”‚ â”‚ """, (build_id, build_number, job_name))             â”‚     â”‚
â”‚ â”‚                                                        â”‚     â”‚
â”‚ â”‚ # 2. Fetch GitHub files                              â”‚     â”‚
â”‚ â”‚ github_files = fetch_github_context(commit_sha)      â”‚     â”‚
â”‚ â”‚ test_scripts = fetch_test_files(repo, branch)        â”‚     â”‚
â”‚ â”‚ knowledge_docs = fetch_readme_docs(repo)             â”‚     â”‚
â”‚ â”‚                                                        â”‚     â”‚
â”‚ â”‚ # 3. Store in MongoDB                                 â”‚     â”‚
â”‚ â”‚ mongodb = pymongo.MongoClient(MONGODB_URI)           â”‚     â”‚
â”‚ â”‚ db = mongodb['jenkins_failure_analysis']             â”‚     â”‚
â”‚ â”‚                                                        â”‚     â”‚
â”‚ â”‚ db.build_context.insert_one({                        â”‚     â”‚
â”‚ â”‚   'build_id': build_id,                              â”‚     â”‚
â”‚ â”‚   'console_log': console_output,                     â”‚     â”‚
â”‚ â”‚   'error_details': error_stack_trace,                â”‚     â”‚
â”‚ â”‚   'github_files': github_files,                      â”‚     â”‚
â”‚ â”‚   'test_scripts': test_scripts,                      â”‚     â”‚
â”‚ â”‚   'knowledge_docs': knowledge_docs,                  â”‚     â”‚
â”‚ â”‚   'created_at': datetime.now()                       â”‚     â”‚
â”‚ â”‚ })                                                    â”‚     â”‚
â”‚ â”‚                                                        â”‚     â”‚
â”‚ â”‚ print("âœ… Build data stored successfully")          â”‚     â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **What Gets Stored Where**

```
PostgreSQL (build_metadata table):
â”œâ”€ build_id: "BUILD_12345"
â”œâ”€ build_number: 12345
â”œâ”€ job_name: "DDN-Storage-Tests"
â”œâ”€ status: "FAILURE"
â”œâ”€ created_at: "2025-01-20 10:00:00"
â”œâ”€ aging_status: "PENDING"
â”œâ”€ aging_days: 0
â”œâ”€ analysis_status: "NOT_ANALYZED"
â”œâ”€ build_url: "https://jenkins.example.com/job/..."
â””â”€ jenkins_url: "https://jenkins.example.com/..."

MongoDB (build_context collection):
â”œâ”€ build_id: "BUILD_12345"
â”œâ”€ console_log: "Full console output (up to 10MB)"
â”œâ”€ error_details:
â”‚  â”œâ”€ error_message: "OutOfMemoryError: Java heap space"
â”‚  â”œâ”€ stack_trace: "Full stack trace..."
â”‚  â””â”€ line_number: 127
â”œâ”€ github_files: [
â”‚  {
â”‚    "filename": "src/main/java/DDNStorage.java",
â”‚    "content": "package com.ddn.storage;...",
â”‚    "commit_sha": "a1b2c3d4e5f6"
â”‚  }
â”‚]
â”œâ”€ test_scripts: [
â”‚  {
â”‚    "filename": "tests/test_storage.py",
â”‚    "content": "import pytest..."
â”‚  }
â”‚]
â”œâ”€ knowledge_docs: [
â”‚  {
â”‚    "filename": "README.md",
â”‚    "content": "# DDN Storage Module..."
â”‚  },
â”‚  {
â”‚    "filename": "docs/architecture.md",
â”‚    "content": "## Architecture Overview..."
â”‚  }
â”‚]
â””â”€ created_at: "2025-01-20T10:00:00Z"

Pinecone (Initially EMPTY for new errors):
â”œâ”€ Only populated AFTER first AI analysis
â””â”€ Then used for future RAG queries
```

---

## ğŸ¯ Where and How RAG Data Is Stored

### **Initial State (Day 0 - Build Fails)**

```
PostgreSQL: âœ… Has build metadata
MongoDB:    âœ… Has full context (logs, code, docs)
Pinecone:   âŒ NO data yet (error is new!)
```

### **After AI Analysis (Day 3 or Manual Trigger)**

```
PostgreSQL: âœ… Updated with analysis results
MongoDB:    âœ… Has analysis_solutions document
Pinecone:   âœ… NOW has vector for this error! â†  For future RAG
```

### **How Error Data Gets into Pinecone for RAG**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STORING ERROR IN PINECONE (After Analysis)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚ After LangGraph + RAG/MCP completes analysis:                   â”‚
â”‚    â†“                                                             â”‚
â”‚ n8n Node: "Store in Pinecone"                                   â”‚
â”‚    POST http://localhost:5003/api/store-vector                  â”‚
â”‚    {                                                             â”‚
â”‚      "text": "OutOfMemoryError: Java heap space at...",        â”‚
â”‚      "metadata": {                                              â”‚
â”‚        "build_id": "BUILD_12345",                               â”‚
â”‚        "error_category": "INFRA_ERROR",                         â”‚
â”‚        "root_cause": "JVM heap size insufficient",              â”‚
â”‚        "solution": "Increase heap to 4GB with -Xmx4g",         â”‚
â”‚        "confidence": 0.95,                                      â”‚
â”‚        "success_rate": 0.0,  â† Initially 0 (not tested yet)   â”‚
â”‚        "times_used": 0,                                         â”‚
â”‚        "timestamp": "2025-01-23T10:00:00Z",                    â”‚
â”‚        "analysis_type": "CLAUDE_MCP"                            â”‚
â”‚      }                                                           â”‚
â”‚    }                                                             â”‚
â”‚    â†“                                                             â”‚
â”‚ Pinecone Storage Service (port 5003):                           â”‚
â”‚    â€¢ Generates embedding via OpenAI                             â”‚
â”‚    â€¢ Stores vector in Pinecone index "ddn-error-solutions"     â”‚
â”‚    â€¢ Vector ID: "BUILD_12345_1729584000"                        â”‚
â”‚    â†“                                                             â”‚
â”‚ âœ… NOW available for future RAG queries!                        â”‚
â”‚                                                                  â”‚
â”‚ Next time similar error occurs:                                 â”‚
â”‚    â€¢ LangGraph searches Pinecone                                â”‚
â”‚    â€¢ Finds this solution (95% similarity)                       â”‚
â”‚    â€¢ Returns it in 5 seconds for $0.01                          â”‚
â”‚    â€¢ NO need for Claude MCP analysis!                           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Files/Documents Attached to RAG**

```
âŒ NOT stored in Pinecone directly:
   â€¢ GitHub code files (too large)
   â€¢ Test scripts (too large)
   â€¢ Knowledge documents (too large)
   â€¢ Full console logs (too large)

âœ… STORED in Pinecone:
   â€¢ Error text â†’ converted to 1536-dim embedding vector
   â€¢ Small metadata:
     - Root cause (text, up to 1000 chars)
     - Solution (text, up to 1000 chars)
     - Success rate (number)
     - Confidence (number)
     - Error category (string)

âœ… STORED in MongoDB:
   â€¢ ALL the large files (code, logs, docs)
   â€¢ Retrieved when needed for MCP analysis
   â€¢ Linked by build_id

Flow for CODE_ERROR (needs GitHub files):
  1. LangGraph classifies as CODE_ERROR
  2. Searches Pinecone (RAG)
  3. If low similarity â†’ needs MCP
  4. Fetches GitHub files from MongoDB (or via GitHub MCP)
  5. Claude analyzes with full context
  6. Solution stored in Pinecone for next time
```

---

## ğŸ”‘ Key Architecture Points

### **1. Separation of Concerns**

```
API Scripts (Jenkins Post-Build):
   â€¢ Responsible for: Data collection and storage
   â€¢ Stores in: PostgreSQL + MongoDB
   â€¢ No AI/analysis logic
   â€¢ Fast and simple

n8n Workflows:
   â€¢ Responsible for: Orchestration and routing
   â€¢ Calls: LangGraph, MCP servers, APIs
   â€¢ Makes decisions: RAG vs MCP path
   â€¢ Handles notifications

LangGraph Service (port 5000):
   â€¢ Responsible for: Classification and RAG search
   â€¢ Uses: OpenAI embeddings, Pinecone
   â€¢ Returns: Error category, similar solutions

MCP Servers (ports 5001, 5002):
   â€¢ Responsible for: Deep analysis tooling
   â€¢ MongoDB MCP: Fetches full logs, stack traces
   â€¢ GitHub MCP: Fetches source code
   â€¢ Only used for CODE_ERROR / TEST_FAILURE
```

### **2. Two Trigger Paths**

```
Path 1: Automatic (3-day aging)
   â”œâ”€ Cron job checks PostgreSQL daily
   â”œâ”€ Updates aging_days counter
   â”œâ”€ When aging_days = 3 â†’ calls n8n webhook
   â””â”€ Good for: Batching, cost savings

Path 2: Manual (Dashboard)
   â”œâ”€ User clicks "Analyze Now" in dashboard
   â”œâ”€ Dashboard API â†’ Manual Trigger API â†’ n8n webhook
   â”œâ”€ Bypasses all aging checks
   â””â”€ Good for: Urgent issues, production failures
```

### **3. Data Flow Summary**

```
1. Jenkins fails â†’ API script stores data â†’ PostgreSQL + MongoDB
2. Wait 3 days OR manual trigger â†’ n8n webhook called
3. n8n fetches data from MongoDB â†’ calls LangGraph
4. LangGraph classifies + searches Pinecone (RAG)
5. Decision: RAG (fast) or MCP (deep)
6. Solution stored in MongoDB + Pinecone
7. Notifications sent to Teams + Dashboard updated
```

---

## âœ… Summary - Are We Aligned Now?

### **What You Told Me (Corrected)**

1. âœ… **Data storage via API scripts** (NOT n8n webhooks)
   - Jenkins post-build script calls Python/Shell API
   - API stores in PostgreSQL + MongoDB
   - Includes GitHub code, test scripts, knowledge docs

2. âœ… **Manual trigger from Dashboard** (NO 3-day wait)
   - User clicks "Analyze Now" button
   - Dashboard API (port 5005) â†’ Manual Trigger API (port 5004)
   - Manual Trigger API calls n8n webhook
   - Analysis starts immediately

3. âœ… **RAG error data attachment**
   - Error text â†’ Pinecone (as embedding vector)
   - Large files â†’ MongoDB (GitHub code, test scripts, docs)
   - Pinecone metadata has solution details
   - MongoDB has complete context

### **Workflow Flow**

```
Jenkins Fail
   â†“
API Script (store data)
   â†“
PostgreSQL + MongoDB (build data + full context)
   â†“
[EITHER]
   â€¢ Wait 3 days (aging) â†’ Cron job â†’ n8n webhook
   â€¢ OR Manual trigger â†’ Dashboard â†’ Manual Trigger API â†’ n8n webhook
   â†“
n8n Orchestration
   â†“
Fetch from MongoDB â†’ LangGraph â†’ RAG/MCP â†’ Solution
   â†“
Store in MongoDB + Pinecone â†’ Notify Teams + Dashboard
```

---

## â“ Questions for You

1. **API Script Location**
   - Do you have the actual Jenkins post-build script?
   - Is it Python, Shell, or Groovy?
   - Can you share it so I can document the exact flow?

2. **GitHub Data Fetching**
   - How does the API script fetch GitHub code?
   - Via GitHub API directly?
   - Or does it clone the repo locally?

3. **Knowledge Documents**
   - Which specific docs are included?
   - Just README.md?
   - All .md files in docs/ folder?
   - Configurable?

4. **Manual Trigger UI**
   - Is the Dashboard UI already built?
   - React/Vue/Angular?
   - Can you show me a screenshot?

5. **3-Day Aging**
   - Is this the right duration?
   - Should it be configurable per job type?
   - Should critical failures skip aging entirely?

---

**We are now on the same page!** This document reflects the ACTUAL architecture with:
- âœ… Data storage via API scripts (not n8n)
- âœ… Manual trigger from dashboard (bypass aging)
- âœ… RAG data in Pinecone, full context in MongoDB

Let me know if this matches your understanding or if there are more corrections needed!
