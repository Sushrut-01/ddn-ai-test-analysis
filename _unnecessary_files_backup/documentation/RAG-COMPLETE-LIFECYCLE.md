# RAG Complete Lifecycle - Build-Specific Data & Updates

**Document**: Complete explanation of RAG lifecycle with updates
**Diagram**: `RAG-Technical-Deep-Dive.jpg` (5.9MB, 14400x10800px, ULTRA CLEAR!)
**Date**: October 21, 2025
**Status**: Final Production Documentation

---

## ğŸ¯ Understanding the Complete Picture

This document answers your key questions:
1. âœ… How each build has its own unique data
2. âœ… How knowledge documents always stay fresh from GitHub
3. âœ… How test scripts change with each build
4. âœ… How n8n workflow updates the system after execution
5. âœ… How updates help future RAG analysis
6. âœ… How dashboard updates in real-time

---

## ğŸ“Š The New Diagram (NO Overlaps!)

**File**: `RAG-Technical-Deep-Dive.jpg`
- **Size**: 5.9 MB
- **Resolution**: 14400x10800 pixels (300 DPI)
- **Canvas**: 48x36 (EXTRA LARGE for perfect clarity!)
- **Sections**: 6 major phases + feedback loop

---

## ğŸ”„ PHASE 1: Each Build is Unique

### **Key Concept: Build Isolation**

Every build is **completely separate** from other builds. This is CRITICAL to understand!

```
BUILD #12345 (Monday 10:00 AM)
â”œâ”€â”€ Commit SHA: a1b2c3d
â”œâ”€â”€ Error: OutOfMemoryError
â”œâ”€â”€ Test Scripts: v1.2.3
â”œâ”€â”€ Knowledge Docs: README v2.1, Setup Guide v1.5
â”œâ”€â”€ Console logs: 8.5 MB (specific to THIS build)
â”œâ”€â”€ XML reports: 15 tests, 3 failures (THIS build's tests)
â””â”€â”€ Stored in MongoDB as: document with _id="BUILD_12345"

BUILD #12346 (Monday 2:00 PM - 4 hours later)
â”œâ”€â”€ Commit SHA: d4e5f6g (DIFFERENT!)
â”œâ”€â”€ Error: NullPointerException (DIFFERENT error!)
â”œâ”€â”€ Test Scripts: v1.2.4 (UPDATED!)
â”œâ”€â”€ Knowledge Docs: README v2.2 (UPDATED!), Setup Guide v1.6 (UPDATED!)
â”œâ”€â”€ Console logs: 9.2 MB (DIFFERENT logs)
â”œâ”€â”€ XML reports: 18 tests, 1 failure (NEW tests added!)
â””â”€â”€ Stored in MongoDB as: SEPARATE document _id="BUILD_12346"

BUILD #12347 (Tuesday 9:00 AM - next day)
â”œâ”€â”€ Commit SHA: g7h8i9j (DIFFERENT again!)
â”œâ”€â”€ Error: OutOfMemoryError (SAME type as #12345, but different context!)
â”œâ”€â”€ Test Scripts: v1.2.5 (UPDATED again!)
â”œâ”€â”€ Knowledge Docs: README v2.3 (UPDATED!), Setup Guide v1.7, NEW: Troubleshooting Guide v1.0
â”œâ”€â”€ Console logs: 7.8 MB (DIFFERENT)
â”œâ”€â”€ XML reports: 20 tests, 2 failures (MORE new tests!)
â””â”€â”€ Stored in MongoDB as: SEPARATE document _id="BUILD_12347"
```

### **Why This Matters for RAG:**

**Question**: BUILD #12347 has the same error (OutOfMemoryError) as BUILD #12345. Will RAG find the #12345 solution?

**Answer**: YES! But here's how:

1. **RAG searches by ERROR SIMILARITY, not exact match**
   ```
   BUILD #12345 error text: "OutOfMemoryError: Java heap space at DDNStorage.java:127"
   BUILD #12347 error text: "OutOfMemoryError: Java heap space at DDNStorage.java:134"

   Embedding similarity: 0.96 (very similar!)

   RAG finds BUILD #12345's solution even though:
   - Different commit
   - Different line number (127 vs 134)
   - Different knowledge doc versions
   - 2 days apart
   ```

2. **RAG considers KNOWLEDGE DOC VERSION**
   ```
   BUILD #12345 solution used README v2.1
   BUILD #12347 has README v2.3

   IF README v2.3 has UPDATED solution:
     RAG will see lower success rate for old solution
     RAG will wait for NEW solution to be added

   IF README v2.3 has SAME solution:
     RAG uses the proven solution from #12345
   ```

3. **RAG stores VERSION INFO in Pinecone**
   ```json
   {
     "vector_id": "BUILD_12345_ts",
     "metadata": {
       "solution": "Increase heap to 4GB",
       "knowledge_doc_version": "v2.1",
       "test_script_version": "v1.2.3",
       "success_rate": 0.92,
       "times_used": 25
     }
   }
   ```

---

## ğŸ“š PHASE 2: Knowledge Documents Always Fresh

### **How Knowledge Docs Update Per Build**

**The Process:**

```
Developer commits code to GitHub
    â†“
GitHub commit includes:
    â€¢ Code changes (DDNStorage.java)
    â€¢ UPDATED README.md (v2.1 â†’ v2.2)
    â€¢ UPDATED setup-guide.md (v1.5 â†’ v1.6)
    â€¢ NEW troubleshooting-guide.md (created!)
    â†“
Jenkins triggers build
    â†“
Build fails
    â†“
API script runs:
    1. Fetches commit SHA: d4e5f6g
    2. Calls GitHub API: GET /repos/{owner}/{repo}/contents/README.md?ref=d4e5f6g
    3. Gets README.md at THIS exact commit (v2.2)
    4. Calls GitHub API for ALL docs in /docs folder
    5. Gets setup-guide.md, troubleshooting-guide.md, etc.
    â†“
Stores in MongoDB:
    {
      "_id": "BUILD_12346",
      "knowledge_docs": {
        "README.md": {
          "version": "v2.2",
          "content": "# DDN Storage\n\n## Memory Requirements\nNow requires 6GB minimum (updated from 4GB)...",
          "commit": "d4e5f6g",
          "last_modified": "2025-10-21T14:00:00Z"
        },
        "setup-guide.md": {
          "version": "v1.6",
          "content": "...",
          "commit": "d4e5f6g"
        },
        "troubleshooting-guide.md": {
          "version": "v1.0",
          "content": "NEW FILE! ## OutOfMemoryError troubleshooting...",
          "commit": "d4e5f6g",
          "is_new": true
        }
      }
    }
```

### **Why This Matters:**

**Scenario**: README updated with NEW solution

```
BUILD #12345 (uses README v2.1):
  README v2.1 says: "OutOfMemoryError: Increase heap to 4GB"
  Solution stored in Pinecone: "Increase to 4GB"
  Success rate after 25 uses: 92%

Developer discovers 4GB is not enough for new features!
Updates README v2.2: "OutOfMemoryError: NOW requires 6GB minimum"

BUILD #12346 (uses README v2.2):
  Fails with OutOfMemoryError
  RAG searches Pinecone
  Finds BUILD #12345 solution: "Increase to 4GB"
  BUT knowledge doc says v2.2: "NOW requires 6GB"

  SMART RAG logic:
    IF knowledge_doc_version_in_pinecone < current_knowledge_doc_version:
      AND knowledge docs mention UPDATED requirement:
        Mark solution as "possibly outdated"
        Reduce confidence score
        Suggest MCP analysis instead

  Result: MCP analyzes with LATEST README v2.2
          Finds correct solution: "Increase to 6GB"
          Stores in Pinecone with knowledge_doc_version="v2.2"

BUILD #12347 (next build, also uses README v2.2):
  RAG now finds BUILD #12346 solution: "Increase to 6GB"
  knowledge_doc_version matches: v2.2
  High confidence: Use this solution!
```

---

## ğŸ§ª PHASE 3: Test Scripts Change Over Time

### **How Test Scripts Evolve**

**Example Timeline:**

```
WEEK 1: Test Scripts v1.2.3
â”œâ”€â”€ test_initialization.py
â”œâ”€â”€ test_read_write.py
â””â”€â”€ 15 total tests

Developer adds new feature: "Large file support"

WEEK 2: Test Scripts v1.2.4
â”œâ”€â”€ test_initialization.py (unchanged)
â”œâ”€â”€ test_read_write.py (unchanged)
â”œâ”€â”€ test_large_files.py (NEW!)
â””â”€â”€ 18 total tests (3 new tests)

BUILD #12346 runs with v1.2.4:
  â€¢ Existing 15 tests: PASS
  â€¢ New 3 tests: FAIL (OutOfMemoryError)

XML Report shows:
  <testsuite tests="18" failures="3">
    <testcase name="test_large_file_1GB" status="FAILED">
      <error>OutOfMemoryError: Java heap space</error>
    </testcase>
  </testsuite>

MongoDB stores:
  {
    "_id": "BUILD_12346",
    "test_results": {
      "total_tests": 18,
      "new_tests_added": ["test_large_file_1GB", "test_large_file_5GB", "test_large_file_10GB"],
      "test_script_version": "v1.2.4",
      "failures": [
        {
          "test_name": "test_large_file_1GB",
          "error": "OutOfMemoryError: Java heap space",
          "is_new_test": true  â† IMPORTANT!
        }
      ]
    }
  }
```

### **How This Affects RAG:**

```
RAG Analysis for BUILD #12346:

1. Sees error: "OutOfMemoryError in test_large_file_1GB"

2. Searches Pinecone for similar errors

3. Finds BUILD #11000: "OutOfMemoryError in test_initialization"
   - Similar error type: OutOfMemoryError
   - Different test name
   - Similarity: 0.85 (good, but not perfect)
   - Solution: "Increase heap to 4GB"

4. SMART CHECK:
   IF error_test_name NOT IN historical_test_names:
     This is a NEW test (test_large_file_1GB)
     Old solutions might not apply
     Reduce confidence

5. Decision:
   - Similarity: 0.85 (borderline)
   - New test: Yes (confidence penalty)
   - Final decision: Use MCP for deeper analysis

6. MCP analyzes:
   - Sees NEW test for large files
   - Checks test code: Creates 1GB file
   - Realizes 4GB heap not enough for 1GB file + overhead
   - Recommends: "Increase to 8GB for large file tests"

7. Solution stored in Pinecone:
   {
     "test_context": "test_large_file_1GB (new in v1.2.4)",
     "solution": "Increase heap to 8GB",
     "test_script_version": "v1.2.4"
   }
```

---

## ğŸ”„ PHASE 4: Workflow Completion Updates (5 Systems)

### **What Happens AFTER n8n Completes Analysis**

This is the CRITICAL part you asked about! After the workflow finishes, n8n UPDATES 5 different systems:

```
n8n Workflow Complete (5 seconds later)
    â†“
    UPDATE 1: MongoDB
    UPDATE 2: Pinecone
    UPDATE 3: PostgreSQL
    UPDATE 4: Dashboard API
    UPDATE 5: Knowledge Base (optional)
```

### **UPDATE 1: MongoDB - Store Complete Solution**

**n8n Node: "Store Solution in MongoDB"**

```javascript
// n8n JavaScript Code Node

const solution = {
  "_id": `${buildId}_solution`,
  "build_id": buildId,
  "analysis_type": "RAG",  // or "MCP"
  "error_category": "INFRA_ERROR",
  "root_cause": "JVM heap size insufficient for large file tests",
  "solution": "Increase JVM heap to 8GB",
  "fix_command": "export JAVA_OPTS='-Xms4g -Xmx8g'",
  "confidence": 0.95,
  "success_rate": 0.0,  // Not tested yet
  "times_used": 0,
  "analyzed_at": new Date(),
  "analysis_duration_ms": 5000,
  "cost_usd": 0.01,

  // Context for future reference
  "knowledge_docs_used": {
    "README.md": "v2.2",
    "troubleshooting-guide.md": "v1.0"
  },
  "test_script_version": "v1.2.4",
  "commit_sha": "d4e5f6g",

  // For tracking
  "similar_builds": ["BUILD_12345", "BUILD_11000"],
  "pinecone_vector_id": "BUILD_12346_1729584500"
};

// MongoDB insert
db.collection('analysis_solutions').insertOne(solution);
```

**Why this matters:**
- Complete audit trail
- Future builds can reference this
- Can be queried independently of Pinecone

### **UPDATE 2: Pinecone - Add to Historical Data**

**n8n Node: "Update Pinecone Vector Database"**

```python
# n8n Python Code Node

from pinecone import Pinecone
from openai import OpenAI

# Generate embedding
openai_client = OpenAI(api_key=OPENAI_KEY)
embedding_response = openai_client.embeddings.create(
    model="text-embedding-3-small",
    input=error_text  # "OutOfMemoryError: Java heap space in test_large_file_1GB"
)
embedding = embedding_response.data[0].embedding

# Store in Pinecone
pc = Pinecone(api_key=PINECONE_KEY)
index = pc.Index("ddn-error-solutions")

vector_id = f"BUILD_{build_id}_{int(time.time())}"

index.upsert(vectors=[{
    "id": vector_id,
    "values": embedding,  # [1536 floats]
    "metadata": {
        "build_id": build_id,
        "error_category": "INFRA_ERROR",
        "error_text": error_text,
        "root_cause": "JVM heap insufficient",
        "solution": "Increase heap to 8GB",
        "fix_command": "export JAVA_OPTS='-Xms4g -Xmx8g'",
        "confidence": 0.95,
        "success_rate": 0.0,  # Will be updated with feedback
        "times_used": 0,
        "times_successful": 0,
        "knowledge_doc_version": "v2.2",
        "test_script_version": "v1.2.4",
        "commit_sha": "d4e5f6g",
        "created_at": datetime.now().isoformat(),
        "last_used": None
    }
}])

print(f"âœ“ Stored in Pinecone: {vector_id}")
```

**Why this matters:**
- **NEXT build with similar error will find THIS solution!**
- Historical data GROWS automatically
- No manual intervention needed
- System learns from every build

### **UPDATE 3: PostgreSQL - Update Build Status**

**n8n Node: "Update Build Record"**

```sql
-- n8n PostgreSQL Node

UPDATE builds
SET
    analysis_status = 'ANALYZED',
    analysis_type = 'RAG',
    analyzed_at = NOW(),
    aging_days = EXTRACT(DAY FROM (NOW() - created_at)),
    root_cause = 'JVM heap insufficient',
    fix_recommendation = 'Increase heap to 8GB',
    confidence_score = 0.95,
    analysis_duration_seconds = 5,
    cost_usd = 0.01
WHERE
    build_id = '12346';

-- Also insert into failure_analysis table
INSERT INTO failure_analysis (
    build_id,
    error_category,
    root_cause,
    fix_recommendation,
    confidence_score,
    success_rate,
    analysis_type,
    created_at
) VALUES (
    '12346',
    'INFRA_ERROR',
    'JVM heap insufficient',
    'Increase heap to 8GB',
    0.95,
    0.0,
    'RAG',
    NOW()
);
```

**Why this matters:**
- Quick status queries from dashboard
- Aging criteria tracking
- Audit trail
- Can generate reports

### **UPDATE 4: Dashboard API - Real-Time Update**

**n8n Node: "Notify Dashboard"**

```javascript
// n8n HTTP Request Node

POST http://localhost:5005/api/builds/12346/update
Content-Type: application/json

{
  "status": "ANALYZED",
  "analysis_type": "RAG",
  "analyzed_at": "2025-10-21T14:05:00Z",
  "duration_seconds": 5,
  "cost_usd": 0.01,
  "solution": {
    "root_cause": "JVM heap size insufficient for large file tests",
    "fix": "Increase JVM heap to 8GB",
    "command": "export JAVA_OPTS='-Xms4g -Xmx8g'",
    "confidence": 0.95,
    "success_rate": 0.0,
    "knowledge_docs_used": ["README v2.2", "Troubleshooting Guide v1.0"]
  },
  "metadata": {
    "similar_builds": ["BUILD_12345", "BUILD_11000"],
    "test_script_version": "v1.2.4"
  }
}
```

**Dashboard receives and:**
1. Updates build status badge: "FAILED" â†’ "ANALYZED"
2. Shows solution card with all details
3. Displays confidence and success rate
4. Provides [Apply Fix] button
5. Provides [This worked] / [Didn't work] feedback buttons
6. If WebSocket connected: Pushes update immediately (no refresh needed!)

**Why this matters:**
- User sees result instantly
- No need to refresh page
- Can immediately test solution
- Provides feedback mechanism

### **UPDATE 5: Knowledge Base (Optional)**

**n8n Node: "Update Knowledge Index" (conditional)**

```javascript
// Only runs if new pattern discovered

IF (isNewErrorPattern || knowledgeDocsUpdated) {

  // Update in-memory error catalog
  POST http://localhost:5001/api/knowledge/update
  {
    "action": "add_pattern",
    "error_pattern": {
      "category": "INFRA_ERROR",
      "keywords": ["heap", "large file", "test_large_file"],
      "typical_solution": "Increase heap for large file operations"
    }
  }

  // Refresh knowledge docs cache
  POST http://localhost:5001/api/knowledge/refresh-docs
  {
    "commit_sha": "d4e5f6g",
    "updated_files": ["README.md", "troubleshooting-guide.md"]
  }
}
```

**Why this matters:**
- Classification improves over time
- New patterns automatically recognized
- Knowledge docs always current in cache

---

## ğŸ”„ PHASE 5: How Updates Help Future Builds

### **Scenario: BUILD #12347 (2 days later)**

```
BUILD #12347 fails with OutOfMemoryError
    â†“
n8n workflow starts
    â†“
LangGraph Classification:
    Error: "OutOfMemoryError: Java heap space in test_large_file_5GB"
    Category: INFRA_ERROR
    â†“
Generate Embedding:
    OpenAI API â†’ [1536 floats]
    â†“
Search Pinecone:
    Query with embedding
    Filter: error_category = "INFRA_ERROR"
    â†“
Results Found:
    1. BUILD #12346 (similarity: 0.97) â† FOUND THE NEW SOLUTION!
       Solution: "Increase to 8GB"
       Knowledge docs: v2.2
       Test scripts: v1.2.4
       Success rate: 0.0 (not tested yet)
       Confidence: 0.95

    2. BUILD #12345 (similarity: 0.89)
       Solution: "Increase to 4GB"
       Knowledge docs: v2.1
       Test scripts: v1.2.3
       Success rate: 0.92 (proven)
       Confidence: 0.95
    â†“
Decision:
    COMPARE the two solutions:

    BUILD #12346:
      + Higher similarity (0.97 vs 0.89)
      + Same test type (large_file)
      + Same knowledge doc version (v2.2)
      + Same test script version (v1.2.4)
      - Not tested yet (success_rate = 0.0)

    BUILD #12345:
      - Lower similarity
      - Different test type
      - Older knowledge docs
      - Older test scripts
      + Proven success (92%)

    SMART RAG LOGIC:
      IF similarity_difference > 0.05:  # 0.97 - 0.89 = 0.08 > 0.05
        AND same_context (knowledge_docs, test_scripts):
          Prefer the more similar solution (BUILD #12346)
          Even though not tested yet
          Reason: Same context = likely to work

    â†“
Result: Use BUILD #12346 solution
    Solution: "Increase to 8GB"
    Time: 5 seconds
    Cost: $0.01
    â†“
User tests it:
    âœ“ IT WORKS!
    â†“
User clicks "This worked" button in dashboard
    â†“
Feedback API updates Pinecone:
    BUILD #12346 vector:
      success_rate: 0.0 â†’ 1.0 (100%!)
      times_used: 0 â†’ 1
      times_successful: 0 â†’ 1
      last_used: "2025-10-23T09:15:00Z"
    â†“
BUILD #12348 (next build, same error):
    RAG finds BUILD #12346:
      Similarity: 0.97
      Success rate: 1.0 (proven!)
      Times used: 1

    Decision: High confidence! Use this solution.
    Result: Works again!

    User feedback: "This worked"
    â†“
    Pinecone update:
      success_rate: 1.0 â†’ 1.0 (still 100%)
      times_used: 1 â†’ 2
      times_successful: 1 â†’ 2
```

### **The Learning Curve:**

```
BUILD #12345:
  Solution: "Increase to 4GB"
  success_rate: 0.92 (after 25 uses)

BUILD #12346 (new context):
  Solution: "Increase to 8GB"
  success_rate: 0.0 â†’ 1.0 â†’ 1.0 â†’ 0.95 â†’ ...
  Times used: 0 â†’ 1 â†’ 2 â†’ 3 â†’ ... â†’ 20

After 20 uses:
  success_rate: 0.95 (proven in new context!)

Future builds:
  RAG has TWO good solutions:
    1. Old context (4GB) - 92% success
    2. New context (8GB) - 95% success

  RAG picks based on similarity and context match
```

---

## ğŸ“Š PHASE 6: Dashboard Real-Time Updates

### **Dashboard Update Flow:**

```
USER PERSPECTIVE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

T+0 seconds: Build fails
  Dashboard shows:
    ğŸ”´ BUILD #12346
    Status: FAILED
    "Pending analysis (3-day aging)"
    [ Analyze Now ] button visible

T+5 seconds: User clicks "Analyze Now"
  Dashboard shows:
    ğŸŸ¡ BUILD #12346
    Status: ANALYZING...
    Progress bar: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 50%
    "Running RAG analysis..."

T+10 seconds: n8n workflow completes
  n8n calls Dashboard API
  Dashboard WebSocket pushes update

  Dashboard shows (NO page refresh!):
    ğŸŸ¢ BUILD #12346
    Status: ANALYZED (RAG)

    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  SOLUTION FOUND                      â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘  Root Cause:                         â•‘
    â•‘  JVM heap insufficient for large     â•‘
    â•‘  file tests                          â•‘
    â•‘                                      â•‘
    â•‘  Fix:                                â•‘
    â•‘  Increase JVM heap to 8GB            â•‘
    â•‘                                      â•‘
    â•‘  Command:                            â•‘
    â•‘  export JAVA_OPTS='-Xms4g -Xmx8g'   â•‘
    â•‘                                      â•‘
    â•‘  Confidence: 95%                     â•‘
    â•‘  Success Rate: N/A (not tested yet)  â•‘
    â•‘  Analysis Time: 5s                   â•‘
    â•‘  Cost: $0.01                         â•‘
    â•‘                                      â•‘
    â•‘  Similar to:                         â•‘
    â•‘  â€¢ BUILD #12345 (0.89 match)        â•‘
    â•‘  â€¢ BUILD #11000 (0.85 match)        â•‘
    â•‘                                      â•‘
    â•‘  [ Apply Fix ]  [ Copy Command ]    â•‘
    â•‘  [ This Worked ] [ Didn't Work ]    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

T+15 seconds: User clicks "Apply Fix"
  Dashboard opens terminal/Jenkins job
  Pre-fills command
  User executes

T+20 seconds: User clicks "This Worked"
  Dashboard sends feedback to API
  API updates Pinecone
  Success rate updates in real-time

  Dashboard updates card:
    Success Rate: N/A â†’ 100% âœ“
    Times Used: 1
```

### **Technical Implementation:**

**Dashboard Frontend (React):**

```jsx
// Dashboard component
function BuildDashboard() {
  const [builds, setBuilds] = useState([]);

  // WebSocket connection for real-time updates
  useEffect(() => {
    const ws = new WebSocket('ws://localhost:5005/ws');

    ws.onmessage = (event) => {
      const update = JSON.parse(event.data);

      if (update.type === 'BUILD_STATUS_UPDATE') {
        // Update build in state without page refresh
        setBuilds(prev => prev.map(build =>
          build.id === update.build_id
            ? { ...build, ...update.data }
            : build
        ));
      }
    };

    return () => ws.close();
  }, []);

  return (
    <div>
      {builds.map(build => (
        <BuildCard
          key={build.id}
          build={build}
          onAnalyzeNow={handleAnalyzeNow}
          onFeedback={handleFeedback}
        />
      ))}
    </div>
  );
}
```

**Dashboard Backend (WebSocket):**

```python
# dashboard_api.py

from fastapi import FastAPI, WebSocket
from fastapi.websockets import WebSocketManager

app = FastAPI()
ws_manager = WebSocketManager()

@app.post("/api/builds/{build_id}/update")
async def update_build(build_id: str, update: dict):
    """
    Called by n8n when analysis completes
    """
    # Store in database
    db.builds.update_one(
        {"build_id": build_id},
        {"$set": update}
    )

    # Push to all connected WebSocket clients
    await ws_manager.broadcast({
        "type": "BUILD_STATUS_UPDATE",
        "build_id": build_id,
        "data": update
    })

    return {"status": "updated"}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket connection for real-time updates
    """
    await websocket.accept()
    await ws_manager.connect(websocket)

    try:
        while True:
            # Keep connection alive
            await websocket.receive_text()
    except:
        await ws_manager.disconnect(websocket)
```

---

## ğŸ¯ Summary: The Complete Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BUILD FAILS â†’ Each build has UNIQUE data                    â”‚
â”‚   â€¢ Unique commit SHA                                       â”‚
â”‚   â€¢ Unique knowledge docs (LATEST from GitHub)              â”‚
â”‚   â€¢ Unique test scripts (version changes)                   â”‚
â”‚   â€¢ Stored separately in MongoDB                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG ANALYSIS â†’ Uses build-specific data                     â”‚
â”‚   â€¢ Fetches THIS build's documents                          â”‚
â”‚   â€¢ Searches Pinecone for similar historical errors         â”‚
â”‚   â€¢ Considers knowledge doc versions                        â”‚
â”‚   â€¢ Decides: RAG (80%) or MCP (20%)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WORKFLOW COMPLETES â†’ Updates 5 systems                      â”‚
â”‚   1. MongoDB: Complete solution stored                      â”‚
â”‚   2. Pinecone: New vector added to historical data          â”‚
â”‚   3. PostgreSQL: Build status updated                       â”‚
â”‚   4. Dashboard: Real-time UI update (WebSocket)             â”‚
â”‚   5. Knowledge Base: Patterns updated (optional)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FUTURE BUILDS BENEFIT â†’ RAG gets smarter                    â”‚
â”‚   â€¢ Historical data grows (Pinecone has more vectors)       â”‚
â”‚   â€¢ Success rates improve (user feedback)                   â”‚
â”‚   â€¢ Knowledge docs stay current (always latest)             â”‚
â”‚   â€¢ Test context preserved (version tracking)               â”‚
â”‚   â€¢ Similar errors resolve faster (5s instead of 18s)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FEEDBACK LOOP â†’ Continuous improvement                      â”‚
â”‚   â€¢ User tests solution                                     â”‚
â”‚   â€¢ Clicks "This worked" or "Didn't work"                   â”‚
â”‚   â€¢ Pinecone updates success_rate                           â”‚
â”‚   â€¢ Next build benefits from improved success rate          â”‚
â”‚   â€¢ System learns from every interaction                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… Key Takeaways

1. **Each build is completely isolated**
   - Different commit = different code = different context
   - Knowledge docs ALWAYS fresh from GitHub
   - Test scripts versioned and tracked

2. **n8n updates 5 systems after analysis**
   - MongoDB (complete audit trail)
   - Pinecone (grows historical data)
   - PostgreSQL (fast queries)
   - Dashboard (real-time UI)
   - Knowledge Base (improved patterns)

3. **Updates directly help future builds**
   - More historical data = better RAG matches
   - Success rates improve with feedback
   - Knowledge stays current automatically
   - System learns continuously

4. **Dashboard updates in real-time**
   - WebSocket for instant updates
   - No page refresh needed
   - User sees progress immediately
   - Feedback loop integrated

5. **The system gets smarter over time**
   - Week 1: 50% RAG hit rate
   - Month 1: 70% RAG hit rate
   - Month 3: 80% RAG hit rate
   - Month 6: 85%+ RAG hit rate
   - Lower costs, faster resolutions

---

**The diagram shows all of this visually in 6 clear phases with NO overlapping labels!**

File: `RAG-Technical-Deep-Dive.jpg` (5.9MB, crystal clear, 48x36 canvas)
